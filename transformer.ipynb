{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReonT_YasRSx"
      },
      "source": [
        "# KAIST AI605 Assignment 3: Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_zM93I1etOp"
      },
      "source": [
        "\n",
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.9, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9qwta269rqLQ",
        "outputId": "4e546b38-3277-4c15-fd9a-e156c79d4db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python 3.9.7\n",
            "torch 1.9.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Iterable, Union\n",
        "from platform import python_version\n",
        "from datetime import datetime\n",
        "from itertools import takewhile\n",
        "from string import digits, ascii_lowercase, ascii_uppercase\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "print(\"python\", python_version())\n",
        "print(\"torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iW7A3KX-fD9P"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import datasets\n",
        "except:\n",
        "    !pip install datasets\n",
        "    clear_output()\n",
        "    import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm5eq4NwQZs"
      },
      "source": [
        "## 1. Attention Layer\n",
        "\n",
        "We will first start with going over a few concepts that you learned in your high school statistics class.\n",
        "The variance of a random variable $X$, $\\text{Var}(X)$ is defined as $\\text{E}[(X-\\mu)^2]$ where $\\mu$ is the mean of $X$.\n",
        "Furthermore, given two independent random variables $X$ and $Y$ and a constant $a$,\n",
        "$$ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y),$$\n",
        "$$ \\text{Var}(aX) = a^2\\text{Var}(X),$$\n",
        "$$ \\text{Var}(XY) = \\text{E}[X^2]\\,\\text{E}[Y^2] - (\\text{E}[X])^2(\\text{E}[Y])^2.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me7nHFa6etOs"
      },
      "source": [
        "> **Problem 1.1** *(3 points)*\n",
        "  Suppose we are given two sets of $n$ random variables, $X_1 \\dots X_n$ and $Y_1 \\dots Y_n$,\n",
        "  where all of these $2n$ variables are mutually independent and have a mean of $0$ and a variance of $1$.\n",
        "  Prove that\n",
        "  $$\\text{Var}\\left(\\sum_i^n X_i Y_i\\right) = n.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek2Cuw9PetOs"
      },
      "source": [
        "> **Solution 1.1**\n",
        "  Given that $X_i$ and $Y_i$ are mutually independent and have a mean of 0 and a variance of 1, we can write\n",
        "  $$\\text{E}[X_i] = \\text{E}[Y_i] = 0$$\n",
        "  and\n",
        "  $$\\text{Var}(X_i) = \\text{E}[(X_i - \\mu_i)^2] = \\text{E}[X_i^2] = 1,$$\n",
        "  $$\\text{Var}(Y_i) = \\text{E}[(Y_i - \\mu_i)^2] = \\text{E}[Y_i^2] = 1,$$\n",
        "  for all $i = 1, \\dots, n$ where $\\mu_i$ is the mean of $X_i$ and $Y_i$ respectively.\n",
        "  Therefore,\n",
        "  $$\n",
        "    \\begin{align*}\n",
        "      \\text{Var}\\left(\\sum_i^n X_i Y_i\\right)\n",
        "      &= \\sum_i^n \\text{Var}(X_iY_i) \\\\\n",
        "      &= \\sum_i^n \\left(\\text{E}[X_i^2]\\,\\text{E}[Y_i^2] - (\\text{E}[X_i])^2(\\text{E}[Y_i])^2\\right) \\\\\n",
        "      &= \\sum_i^n (1 \\cdot 1 - 0^2 \\cdot 0^2) \\\\\n",
        "      &= n.\n",
        "    \\end{align*}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIelEM56jn_"
      },
      "source": [
        "In Lecture 11 and 12, we discussed how the attention is computed in Transformer via the following equation,\n",
        "  $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZuqRvaKetOt"
      },
      "source": [
        "> **Problem 1.2** *(3 points)* \n",
        "  Suppose $Q$ and $K$ are matrices of independent variables each of which has a mean of $0$ and a variance of $1$.\n",
        "  Using what you learned from Problem 1.1., show that\n",
        "  $$\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = \\mathbf{1}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0xh-owYetOt"
      },
      "source": [
        "> **Solution 1.2**\n",
        "  Let the matrix $Q \\in \\mathbb{R}^{n \\times d_k}$ and the matrix $K \\in \\mathbb{R}^{m \\times d_m}$.\n",
        "  The $Q_i$ and the $K_j$ are the row vectors of $Q$ and $K$ respectively, and each of them contains $d_k$ random variables.\n",
        "  Total $2d_k$ random variables in $Q_i$ and $K_j$ are mutually independent and have a mean of $0$ and a variance of $1$.\n",
        "  Then, using the result of Problem 1.1, we can rewrite the equation as\n",
        "  $$\n",
        "    \\begin{align*}\n",
        "      \\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\n",
        "      &= \\frac{1}{d_k} \\text{Var}(QK^\\top) \\\\\n",
        "      &= \\frac{1}{d_k} \\text{Var}\\left(\\sum_k^{d_k} Q_{ik} K_{jk}\\right) \\\\\n",
        "      &= \\frac{1}{d_k}\n",
        "        \\begin{bmatrix}\n",
        "          d_k    & \\cdots & d_k \\\\\n",
        "          \\vdots & \\ddots & \\vdots \\\\\n",
        "          d_k    & \\cdots & d_k\n",
        "        \\end{bmatrix} \\\\\n",
        "      &= \\begin{bmatrix}\n",
        "          1      & \\cdots & 1 \\\\\n",
        "          \\vdots & \\ddots & \\vdots \\\\\n",
        "          1      & \\cdots & 1\n",
        "        \\end{bmatrix} \\\\\n",
        "      &= \\mathbf{1}.\n",
        "    \\end{align*}\n",
        "  $$\n",
        "  Here, $\\mathbf{1}$ represents the matrix of ones of size $n \\times m$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU3a3FEu6loq"
      },
      "source": [
        "> **Problem 1.3** *(4 points)*\n",
        "  What would happen if the assumption that the variance of $Q$ and $K$ is $1$ does not hold?\n",
        "  Consider each case of it being higher and lower than $1$ and conjecture what it implies, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4wxrhrUetOu"
      },
      "source": [
        "> **Solution 1.3**\n",
        "  If the variance of $Q$ and $K$ is higher than $1$, values go to the regions that the gradients of the softmax function are small.\n",
        "  It means that the gradients becomes small and it affects negatively while the model is training.\n",
        "  If the variance of $Q$ and $K$ is lower than $1$, the softmax output variance also becomes lower.\n",
        "  It means that the attention values are not much different from each other, and the attention is not able to focus on the important parts of $V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afEcnAG8Q1Jo"
      },
      "source": [
        "## 2. Transformer\n",
        "\n",
        "In this section, you will implement Transformer for a few tasks that are simpler than machine translation.\n",
        "First, go through [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) and make sure you understand every block of the code.\n",
        "Then, you will reuse these code where appropriate to create models for following three tasks.\n",
        "Note that we do not provide a separate training or evaluation data, so it is your job to be able to create these in a reasonable manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9bymUQletOv"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ELTsJGrJetOv"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
        "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 layer_norm_eps: float = 1e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        # Encoder module\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, layer_norm_eps)\n",
        "        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "        # Decoder module\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, layer_norm_eps)\n",
        "        decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
        "        # Reset parameters\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
        "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz: int):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K8zatCsetOw"
      },
      "source": [
        "### Encoder and Decoder Stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "05IPg4UmetOw"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LTrkP8lbetOw"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([deepcopy(decoder_layer) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):\n",
        "        output = tgt\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZqkebCL9etOx"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, layer_norm_eps=1e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        # Attention modules\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        # Feed-forward modules\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        # Encoder modules\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # Self-attention\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        # Feed-forward\n",
        "        src2 = self.linear1(src)\n",
        "        src2 = F.relu(src2)\n",
        "        src2 = self.dropout(src2)\n",
        "        src2 = self.linear2(src2)\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W6sq5uj4etOx"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, layer_norm_eps=1e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        # Attention modules\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        # Feed-forward modules\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        # Decoder modules\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):\n",
        "        # Self-attention\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        # Cross-attention\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        # Feed-forward\n",
        "        tgt2 = self.linear1(tgt)\n",
        "        tgt2 = F.relu(tgt2)\n",
        "        tgt2 = self.dropout(tgt2)\n",
        "        tgt2 = self.linear2(tgt2)\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6AH5BqjetOy"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QbtOij2xetOy"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., **kwargs):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.head_dim_scale = math.sqrt(self.head_dim)\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        # Projection\n",
        "        self.in_proj_weight = nn.Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
        "        self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        # Reset parameters\n",
        "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
        "        nn.init.constant_(self.in_proj_bias, 0.)\n",
        "        nn.init.constant_(self.out_proj.bias, 0.)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n",
        "        batch_size, tgt_len, _ = query.shape\n",
        "        _, src_len, _ = key.shape\n",
        "\n",
        "        w_q, w_k, w_v = self.in_proj_weight.chunk(3)\n",
        "        b_q, b_k, b_v = self.in_proj_bias.chunk(3)\n",
        "        query = F.linear(query.transpose(0, 1), w_q, b_q) \\\n",
        "                 .reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        key   = F.linear(key.transpose(0, 1), w_k, b_k) \\\n",
        "                 .reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        value = F.linear(value.transpose(0, 1), w_v, b_v) \\\n",
        "                 .reshape(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = \\\n",
        "                key_padding_mask.view(batch_size, 1, 1, src_len) \\\n",
        "                                .expand(-1, self.num_heads, -1, -1) \\\n",
        "                                .reshape(batch_size * self.num_heads, 1, src_len)\n",
        "            if attn_mask is None:\n",
        "                attn_mask = key_padding_mask\n",
        "            else:\n",
        "                if attn_mask.dim() == 2:\n",
        "                    attn_mask = attn_mask.unsqueeze(0)\n",
        "                if attn_mask.dtype == torch.bool:\n",
        "                    attn_mask = attn_mask.logical_or(key_padding_mask)\n",
        "                else:\n",
        "                    attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "\n",
        "        if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
        "            attn_mask = torch.zeros_like(attn_mask, dtype=torch.float) \\\n",
        "                             .masked_fill_(attn_mask, float(\"-inf\"))\n",
        "\n",
        "        attn = torch.bmm(query, key.transpose(-2, -1)) / self.head_dim_scale\n",
        "        attn += attn_mask if attn_mask is not None else 0\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "        attn = torch.bmm(attn, value)\n",
        "        attn = attn.transpose(0, 1).reshape(tgt_len, batch_size, self.embed_dim)\n",
        "        attn = self.out_proj(attn).transpose(1, 0)\n",
        "        return attn, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3P5dOvOetOy"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U4Dwz-chetOy"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, step=2) * -(math.log(10000) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vDikD8RetOz"
      },
      "source": [
        "### Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kgDCqcwFetOz"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int = 512, nhead: int = 8,\n",
        "                 num_encoder_layers: int = 6, num_decoder_layers: int = 6,\n",
        "                 dim_feedforward: int = 2048, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model_scale = math.sqrt(d_model)\n",
        "        self.embedder = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
        "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                                       dim_feedforward, dropout, batch_first=True)\n",
        "        self.out_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        src = self.embedder(src) * self.d_model_scale\n",
        "        tgt = self.embedder(tgt) * self.d_model_scale\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "        out = self.transformer(\n",
        "            src, tgt, tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "        )\n",
        "        out = self.out_linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6VIB_metOz"
      },
      "source": [
        "### Optimizer Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O5AvRiwMetOz"
      },
      "outputs": [],
      "source": [
        "class NoamLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_epochs: int, warmup_factor: float = 1.0):\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.warmup_factor = warmup_factor\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        last_epoch = max(1, self.last_epoch)\n",
        "        scale = self.warmup_factor \\\n",
        "              * min(last_epoch ** (-0.5), last_epoch * self.warmup_epochs ** (-1.5))\n",
        "        return [base_lr * scale for base_lr in self.base_lrs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nbChk4FetO0"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qXmtrAouetO0"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss(_Loss):\n",
        "    def __init__(self, ignore_index: int = -100, reduction: str = \"mean\", label_smoothing: float = 0.0):\n",
        "        super().__init__(reduction=reduction)\n",
        "        self.ignore_index = ignore_index\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        is_ignore_index_enabled = (0 <= self.ignore_index < input.size(1))\n",
        "        num_wrong_labels = input.size(1) - (2 if is_ignore_index_enabled else 1)\n",
        "        smoothed_target = torch.full_like(input, self.label_smoothing / num_wrong_labels)\n",
        "        smoothed_target.scatter_(1, target.unsqueeze(1), 1 - self.label_smoothing)\n",
        "        if is_ignore_index_enabled:\n",
        "            smoothed_target[:, self.ignore_index] = 0\n",
        "        out = -torch.sum(F.log_softmax(input, dim=1) * smoothed_target, dim=1)\n",
        "        if self.reduction == \"mean\":\n",
        "            out = torch.mean(out)\n",
        "        elif self.reduction == \"sum\":\n",
        "            out = torch.sum(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJFvZDDwetO0"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vsPCOIkPetO0"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, criterion, optimizer, scheduler, device=\"cuda\"):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "        self.model_state_dict = self.model.state_dict\n",
        "        self.model_load_state_dict = self.model.load_state_dict\n",
        "\n",
        "    def run_epoch(self, dataloader, train=True):\n",
        "        total_loss, total_corrects, total_samples = 0, 0, 0\n",
        "        if train:\n",
        "            self.model.train()\n",
        "            torch.set_grad_enabled(True)\n",
        "        else:\n",
        "            self.model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "\n",
        "        for src, tgt, src_mask, tgt_mask in dataloader:\n",
        "            src, tgt = src.to(self.device), tgt.to(self.device)\n",
        "            tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            src_mask = None if src_mask is None else src_mask.to(self.device)\n",
        "            tgt_mask = None if tgt_mask is None else tgt_mask[:, 1:].to(self.device)\n",
        "\n",
        "            out = self.model(src, tgt_in, src_mask, tgt_mask)\n",
        "            loss = self.criterion(out.permute(0, 2, 1), tgt_out)\n",
        "\n",
        "            if train:\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            pred = torch.argmax(out, dim=2)\n",
        "            corrects = torch.sum(torch.all(pred == tgt_out, dim=1)).item()\n",
        "\n",
        "            total_loss += loss.detach().item() * src.size(0)\n",
        "            total_corrects += corrects\n",
        "            total_samples += src.size(0)\n",
        "\n",
        "        epoch_loss = total_loss / total_samples\n",
        "        epoch_acc = total_corrects / total_samples\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src, src_mask, start_idx, end_idx):\n",
        "        self.model.eval()\n",
        "        pred = torch.full((src.size(0), 1), start_idx, dtype=torch.long, device=self.device)\n",
        "        for _ in range(src.size(1) + 2):\n",
        "            out_t = self.model(src, pred, src_mask, None)\n",
        "            pred_t = torch.argmax(out_t[:, -1, :], dim=1)\n",
        "            pred = torch.hstack((pred, pred_t.unsqueeze(1)))\n",
        "\n",
        "        end_lambda = lambda x: x != end_idx\n",
        "        results = []\n",
        "        for i in range(src.size(0)):\n",
        "            src_str = \"\".join(map(Vocab.to_vocab, takewhile(end_lambda, src[i, 1:].tolist())))\n",
        "            pred_str = \"\".join(map(Vocab.to_vocab, takewhile(end_lambda, pred[i, 1:].tolist())))\n",
        "            results.append((src_str, pred_str))\n",
        "        return results\n",
        "\n",
        "    def train(self, train_loader, valid_loader, num_epochs):\n",
        "        best_loss, best_acc, best_state_dict = float(\"inf\"), 0, None\n",
        "        for e in trange(num_epochs, desc=\"Epoch\", leave=False):\n",
        "            train_loss, train_acc = self.run_epoch(train_loader, train=True)\n",
        "            tqdm.write(f\"Epoch {e+1:3d} Train Loss: {train_loss:.5f} Acc: {train_acc * 100:6.2f}\")\n",
        "            test_loss, test_acc = self.run_epoch(valid_loader, train=False)\n",
        "            tqdm.write(f\"          Test  Loss: {test_loss:.5f} Acc: {test_acc * 100:6.2f}\")\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "            if test_acc > best_acc or (test_acc == best_acc and test_loss < best_loss):\n",
        "                best_loss, best_acc, best_state_dict = test_loss, test_acc, self.model_state_dict()\n",
        "                tqdm.write(f\"          Updated Best Loss: {best_loss:.5f} Best Acc: {best_acc * 100:6.2f}\")\n",
        "\n",
        "        return best_loss, best_acc, best_state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ty_6XBmetO1"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Upo9Pp21etO1"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    valid_vocab_re = re.compile(r\"^[0-9a-zA-Z-' ]+$\")\n",
        "    numeric_vocab_re = re.compile(r\"^[0-9-]+$\")\n",
        "    vocabs = [\"_\", \"[\", \"]\", *digits, *ascii_lowercase, \"-\", \"'\", \" \"]\n",
        "    to_idx_dict = {vocab: idx for idx, vocab in enumerate(vocabs)}\n",
        "    to_vocab_dict = {idx: vocab for idx, vocab in enumerate(vocabs)}\n",
        "    char_vocabs = vocabs[3:]\n",
        "    num_vocabs = len(to_idx_dict)\n",
        "    num_control_vocabs = 3\n",
        "    num_valid_vocabs = num_vocabs - num_control_vocabs\n",
        "    num_char_vocabs = len(char_vocabs)\n",
        "    num_digit_vocabs = 10\n",
        "\n",
        "    @classmethod\n",
        "    def to_idx(cls, char: str) -> int:\n",
        "        return cls.to_idx_dict[char]\n",
        "\n",
        "    @classmethod\n",
        "    def to_vocab(cls, idx: int) -> str:\n",
        "        return cls.to_vocab_dict[idx]\n",
        "\n",
        "    @classmethod\n",
        "    def sample_digits(cls, size: int = 1, generator=None) -> str:\n",
        "        nums = torch.randint(0, 10, size=(size,), generator=generator).tolist()\n",
        "        tokens = \"\".join([str(d) for d in nums])\n",
        "        return tokens\n",
        "\n",
        "    @classmethod\n",
        "    def sample_chars(cls, size: int = 1, generator=None) -> str:\n",
        "        idxs = torch.randint(3, 42, size=(size,), generator=generator).tolist()\n",
        "        tokens = \"\".join([cls.to_vocab(i) for i in idxs])\n",
        "        return tokens\n",
        "\n",
        "    @classmethod\n",
        "    def is_valid_vocabs(cls, vocabs: Union[str, Iterable[str]]) -> bool:\n",
        "        return cls.valid_vocab_re.fullmatch(vocabs) is not None\n",
        "\n",
        "    @classmethod\n",
        "    def is_numeric_vocabs(cls, vocabs: Union[str, Iterable[str]]) -> bool:\n",
        "        return cls.numeric_vocab_re.fullmatch(vocabs) is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gBEtQ8Q9etO1"
      },
      "outputs": [],
      "source": [
        "def get_collate_fn(pad_tensor=None):\n",
        "    if pad_tensor is None:\n",
        "        def collate_fn(batch):\n",
        "            source = torch.vstack([d[0].unsqueeze(dim=0) for d in batch])\n",
        "            target = torch.vstack([d[1].unsqueeze(dim=0) for d in batch])\n",
        "            source_mask = target_mask = None\n",
        "            return source, target, source_mask, target_mask\n",
        "    else:\n",
        "        def collate_fn(batch):\n",
        "            max_len_source = max(len(d[0]) for d in batch)\n",
        "            max_len_target = max(len(d[1]) for d in batch)\n",
        "            source = torch.full((len(batch), max_len_source), pad_tensor, dtype=torch.long)\n",
        "            target = torch.full((len(batch), max_len_target), pad_tensor, dtype=torch.long)\n",
        "            source_mask = torch.full((len(batch), max_len_source), True, dtype=torch.bool)\n",
        "            target_mask = torch.full((len(batch), max_len_target), True, dtype=torch.bool)\n",
        "            for i, d in enumerate(batch):\n",
        "                source[i, :len(d[0])] = d[0]\n",
        "                target[i, :len(d[1])] = d[1]\n",
        "                source_mask[i, :len(d[0])] = False\n",
        "                target_mask[i, :len(d[1])] = False\n",
        "            return source, target, source_mask, target_mask\n",
        "    return collate_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fCbl9PjPetO1"
      },
      "outputs": [],
      "source": [
        "class NLPDataset(Dataset):\n",
        "    pad_token = \"_\"\n",
        "    start_token = \"[\"\n",
        "    end_token = \"]\"\n",
        "    vocab_size = Vocab.num_vocabs\n",
        "    collate_fn = get_collate_fn(pad_tensor=Vocab.to_idx(pad_token))\n",
        "\n",
        "    def __init__(self, seed: int = 109):\n",
        "        self.seed = seed\n",
        "        self.rng = torch.Generator().manual_seed(seed)\n",
        "\n",
        "    def tokens_to_idx(self, tokens: Union[str, Iterable[str]]):\n",
        "        return torch.tensor([Vocab.to_idx(self.start_token)]\n",
        "                          + [Vocab.to_idx(v) for v in tokens]\n",
        "                          + [Vocab.to_idx(self.end_token)], dtype=torch.long)\n",
        "\n",
        "    def idx_to_tokens(self, idx: torch.Tensor):\n",
        "        start_idx = Vocab.to_idx(self.start_token)\n",
        "        end_idx = Vocab.to_idx(self.end_token)\n",
        "        tokens = []\n",
        "        for i in (idx[1:] if idx[0] == start_idx else idx).tolist():\n",
        "            if i == end_idx:\n",
        "                break\n",
        "            tokens.append(Vocab.to_vocab(i))\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.source[idx], self.target[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RhtLmqgetO2"
      },
      "source": [
        "> **Problem 2.1** *(4 points)*\n",
        "  Create a model that takes a random set of input symbols from a vocabulary of digits (i.e. 0, 1, ... , 8, 9) as the input and generate back the same symbols.\n",
        "  Instead of varying length, we fix the length to 32.\n",
        "  Make sure to report that your model's accuracy (gives credit only if the entire output sequence is correct) goes above 90%.\n",
        "  Note that a similar problem is also in Annotated Transformer, and copying code is allowed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G-_oCEQuetO2"
      },
      "outputs": [],
      "source": [
        "class DigitDataset(NLPDataset):\n",
        "    pad_token = None\n",
        "    vocab_size = Vocab.num_control_vocabs + Vocab.num_digit_vocabs\n",
        "    collate_fn = get_collate_fn(pad_tensor=None)\n",
        "\n",
        "    def __init__(self, num_data: int, token_len: int, seed: int = 109):\n",
        "        super().__init__(seed=seed)\n",
        "        data = [self.tokens_to_idx(Vocab.sample_digits(token_len, generator=self.rng))\n",
        "                for _ in range(num_data)]\n",
        "        self.source = self.target = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OxO_0jbFetO2",
        "outputId": "0451d4a9-7a55-4cf3-880d-309074dcfe77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "790f3650d04d4f18bad1f082bc0b95e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/70 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 Train Loss: 2.30001 Acc:   0.00\n",
            "          Test  Loss: 2.16563 Acc:   0.00\n",
            "          Updated Best Loss: 2.16563 Best Acc:   0.00\n",
            "Epoch   2 Train Loss: 2.15899 Acc:   0.00\n",
            "          Test  Loss: 2.06764 Acc:   0.00\n",
            "          Updated Best Loss: 2.06764 Best Acc:   0.00\n",
            "Epoch   3 Train Loss: 2.09246 Acc:   0.00\n",
            "          Test  Loss: 1.99632 Acc:   0.00\n",
            "          Updated Best Loss: 1.99632 Best Acc:   0.00\n",
            "Epoch   4 Train Loss: 2.04703 Acc:   0.00\n",
            "          Test  Loss: 1.94798 Acc:   0.00\n",
            "          Updated Best Loss: 1.94798 Best Acc:   0.00\n",
            "Epoch   5 Train Loss: 2.01505 Acc:   0.00\n",
            "          Test  Loss: 1.90460 Acc:   0.00\n",
            "          Updated Best Loss: 1.90460 Best Acc:   0.00\n",
            "Epoch   6 Train Loss: 1.99106 Acc:   0.00\n",
            "          Test  Loss: 1.86787 Acc:   0.00\n",
            "          Updated Best Loss: 1.86787 Best Acc:   0.00\n",
            "Epoch   7 Train Loss: 1.96579 Acc:   0.00\n",
            "          Test  Loss: 1.83490 Acc:   0.00\n",
            "          Updated Best Loss: 1.83490 Best Acc:   0.00\n",
            "Epoch   8 Train Loss: 1.94658 Acc:   0.00\n",
            "          Test  Loss: 1.79693 Acc:   0.00\n",
            "          Updated Best Loss: 1.79693 Best Acc:   0.00\n",
            "Epoch   9 Train Loss: 1.92862 Acc:   0.00\n",
            "          Test  Loss: 1.79463 Acc:   0.00\n",
            "          Updated Best Loss: 1.79463 Best Acc:   0.00\n",
            "Epoch  10 Train Loss: 1.91339 Acc:   0.00\n",
            "          Test  Loss: 1.75960 Acc:   0.00\n",
            "          Updated Best Loss: 1.75960 Best Acc:   0.00\n",
            "Epoch  11 Train Loss: 1.89834 Acc:   0.00\n",
            "          Test  Loss: 1.72839 Acc:   0.00\n",
            "          Updated Best Loss: 1.72839 Best Acc:   0.00\n",
            "Epoch  12 Train Loss: 1.88228 Acc:   0.00\n",
            "          Test  Loss: 1.72255 Acc:   0.00\n",
            "          Updated Best Loss: 1.72255 Best Acc:   0.00\n",
            "Epoch  13 Train Loss: 1.86564 Acc:   0.00\n",
            "          Test  Loss: 1.68318 Acc:   0.00\n",
            "          Updated Best Loss: 1.68318 Best Acc:   0.00\n",
            "Epoch  14 Train Loss: 1.84818 Acc:   0.00\n",
            "          Test  Loss: 1.65683 Acc:   0.00\n",
            "          Updated Best Loss: 1.65683 Best Acc:   0.00\n",
            "Epoch  15 Train Loss: 1.82752 Acc:   0.00\n",
            "          Test  Loss: 1.63974 Acc:   0.00\n",
            "          Updated Best Loss: 1.63974 Best Acc:   0.00\n",
            "Epoch  16 Train Loss: 1.81006 Acc:   0.00\n",
            "          Test  Loss: 1.58589 Acc:   0.00\n",
            "          Updated Best Loss: 1.58589 Best Acc:   0.00\n",
            "Epoch  17 Train Loss: 1.79070 Acc:   0.00\n",
            "          Test  Loss: 1.56450 Acc:   0.00\n",
            "          Updated Best Loss: 1.56450 Best Acc:   0.00\n",
            "Epoch  18 Train Loss: 1.76616 Acc:   0.00\n",
            "          Test  Loss: 1.52233 Acc:   0.00\n",
            "          Updated Best Loss: 1.52233 Best Acc:   0.00\n",
            "Epoch  19 Train Loss: 1.73545 Acc:   0.00\n",
            "          Test  Loss: 1.47667 Acc:   0.00\n",
            "          Updated Best Loss: 1.47667 Best Acc:   0.00\n",
            "Epoch  20 Train Loss: 1.70640 Acc:   0.00\n",
            "          Test  Loss: 1.43953 Acc:   0.00\n",
            "          Updated Best Loss: 1.43953 Best Acc:   0.00\n",
            "Epoch  21 Train Loss: 1.67357 Acc:   0.00\n",
            "          Test  Loss: 1.34856 Acc:   0.00\n",
            "          Updated Best Loss: 1.34856 Best Acc:   0.00\n",
            "Epoch  22 Train Loss: 1.63034 Acc:   0.00\n",
            "          Test  Loss: 1.28082 Acc:   0.00\n",
            "          Updated Best Loss: 1.28082 Best Acc:   0.00\n",
            "Epoch  23 Train Loss: 1.58879 Acc:   0.00\n",
            "          Test  Loss: 1.25057 Acc:   0.00\n",
            "          Updated Best Loss: 1.25057 Best Acc:   0.00\n",
            "Epoch  24 Train Loss: 1.54001 Acc:   0.00\n",
            "          Test  Loss: 1.15940 Acc:   0.00\n",
            "          Updated Best Loss: 1.15940 Best Acc:   0.00\n",
            "Epoch  25 Train Loss: 1.49347 Acc:   0.00\n",
            "          Test  Loss: 1.08395 Acc:   0.00\n",
            "          Updated Best Loss: 1.08395 Best Acc:   0.00\n",
            "Epoch  26 Train Loss: 1.43964 Acc:   0.00\n",
            "          Test  Loss: 1.03911 Acc:   0.00\n",
            "          Updated Best Loss: 1.03911 Best Acc:   0.00\n",
            "Epoch  27 Train Loss: 1.39758 Acc:   0.00\n",
            "          Test  Loss: 0.94776 Acc:   0.00\n",
            "          Updated Best Loss: 0.94776 Best Acc:   0.00\n",
            "Epoch  28 Train Loss: 1.34920 Acc:   0.00\n",
            "          Test  Loss: 0.91019 Acc:   0.00\n",
            "          Updated Best Loss: 0.91019 Best Acc:   0.00\n",
            "Epoch  29 Train Loss: 1.30346 Acc:   0.00\n",
            "          Test  Loss: 0.85396 Acc:   0.00\n",
            "          Updated Best Loss: 0.85396 Best Acc:   0.00\n",
            "Epoch  30 Train Loss: 1.25296 Acc:   0.00\n",
            "          Test  Loss: 0.75370 Acc:   0.00\n",
            "          Updated Best Loss: 0.75370 Best Acc:   0.00\n",
            "Epoch  31 Train Loss: 1.20869 Acc:   0.00\n",
            "          Test  Loss: 0.76607 Acc:   0.00\n",
            "Epoch  32 Train Loss: 1.15897 Acc:   0.00\n",
            "          Test  Loss: 0.64993 Acc:   0.00\n",
            "          Updated Best Loss: 0.64993 Best Acc:   0.00\n",
            "Epoch  33 Train Loss: 1.11299 Acc:   0.00\n",
            "          Test  Loss: 0.59188 Acc:   0.00\n",
            "          Updated Best Loss: 0.59188 Best Acc:   0.00\n",
            "Epoch  34 Train Loss: 1.06858 Acc:   0.00\n",
            "          Test  Loss: 0.53749 Acc:   0.10\n",
            "          Updated Best Loss: 0.53749 Best Acc:   0.10\n",
            "Epoch  35 Train Loss: 1.02784 Acc:   0.00\n",
            "          Test  Loss: 0.50107 Acc:   0.40\n",
            "          Updated Best Loss: 0.50107 Best Acc:   0.40\n",
            "Epoch  36 Train Loss: 0.97671 Acc:   0.00\n",
            "          Test  Loss: 0.49082 Acc:   0.20\n",
            "Epoch  37 Train Loss: 0.92930 Acc:   0.00\n",
            "          Test  Loss: 0.42213 Acc:   1.20\n",
            "          Updated Best Loss: 0.42213 Best Acc:   1.20\n",
            "Epoch  38 Train Loss: 0.88323 Acc:   0.00\n",
            "          Test  Loss: 0.36574 Acc:   3.50\n",
            "          Updated Best Loss: 0.36574 Best Acc:   3.50\n",
            "Epoch  39 Train Loss: 0.83220 Acc:   0.00\n",
            "          Test  Loss: 0.35627 Acc:   1.20\n",
            "Epoch  40 Train Loss: 0.77696 Acc:   0.00\n",
            "          Test  Loss: 0.28383 Acc:  12.10\n",
            "          Updated Best Loss: 0.28383 Best Acc:  12.10\n",
            "Epoch  41 Train Loss: 0.72057 Acc:   0.00\n",
            "          Test  Loss: 0.23298 Acc:  27.00\n",
            "          Updated Best Loss: 0.23298 Best Acc:  27.00\n",
            "Epoch  42 Train Loss: 0.67309 Acc:   0.00\n",
            "          Test  Loss: 0.19991 Acc:  36.50\n",
            "          Updated Best Loss: 0.19991 Best Acc:  36.50\n",
            "Epoch  43 Train Loss: 0.62068 Acc:   0.00\n",
            "          Test  Loss: 0.16815 Acc:  44.70\n",
            "          Updated Best Loss: 0.16815 Best Acc:  44.70\n",
            "Epoch  44 Train Loss: 0.57270 Acc:   0.00\n",
            "          Test  Loss: 0.13541 Acc:  52.00\n",
            "          Updated Best Loss: 0.13541 Best Acc:  52.00\n",
            "Epoch  45 Train Loss: 0.53460 Acc:   0.04\n",
            "          Test  Loss: 0.11130 Acc:  62.90\n",
            "          Updated Best Loss: 0.11130 Best Acc:  62.90\n",
            "Epoch  46 Train Loss: 0.48654 Acc:   0.07\n",
            "          Test  Loss: 0.08138 Acc:  80.00\n",
            "          Updated Best Loss: 0.08138 Best Acc:  80.00\n",
            "Epoch  47 Train Loss: 0.45406 Acc:   0.16\n",
            "          Test  Loss: 0.05424 Acc:  92.70\n",
            "          Updated Best Loss: 0.05424 Best Acc:  92.70\n",
            "Epoch  48 Train Loss: 0.41188 Acc:   0.34\n",
            "          Test  Loss: 0.05811 Acc:  79.60\n",
            "Epoch  49 Train Loss: 0.37426 Acc:   0.62\n",
            "          Test  Loss: 0.03172 Acc:  95.70\n",
            "          Updated Best Loss: 0.03172 Best Acc:  95.70\n",
            "Epoch  50 Train Loss: 0.34484 Acc:   0.94\n",
            "          Test  Loss: 0.02490 Acc:  96.20\n",
            "          Updated Best Loss: 0.02490 Best Acc:  96.20\n",
            "Epoch  51 Train Loss: 0.30588 Acc:   1.89\n",
            "          Test  Loss: 0.01600 Acc:  98.90\n",
            "          Updated Best Loss: 0.01600 Best Acc:  98.90\n",
            "Epoch  52 Train Loss: 0.27621 Acc:   3.20\n",
            "          Test  Loss: 0.01584 Acc:  97.50\n",
            "Epoch  53 Train Loss: 0.24560 Acc:   4.54\n",
            "          Test  Loss: 0.00932 Acc:  99.10\n",
            "          Updated Best Loss: 0.00932 Best Acc:  99.10\n",
            "Epoch  54 Train Loss: 0.22292 Acc:   6.85\n",
            "          Test  Loss: 0.00585 Acc:  99.80\n",
            "          Updated Best Loss: 0.00585 Best Acc:  99.80\n",
            "Epoch  55 Train Loss: 0.20039 Acc:   9.29\n",
            "          Test  Loss: 0.00627 Acc:  99.80\n",
            "Epoch  56 Train Loss: 0.17837 Acc:  12.70\n",
            "          Test  Loss: 0.00359 Acc: 100.00\n",
            "          Updated Best Loss: 0.00359 Best Acc: 100.00\n",
            "Epoch  57 Train Loss: 0.15846 Acc:  16.56\n",
            "          Test  Loss: 0.00290 Acc:  99.90\n",
            "Epoch  58 Train Loss: 0.14011 Acc:  20.19\n",
            "          Test  Loss: 0.00167 Acc: 100.00\n",
            "          Updated Best Loss: 0.00167 Best Acc: 100.00\n",
            "Epoch  59 Train Loss: 0.12613 Acc:  24.50\n",
            "          Test  Loss: 0.00144 Acc: 100.00\n",
            "          Updated Best Loss: 0.00144 Best Acc: 100.00\n",
            "Epoch  60 Train Loss: 0.11318 Acc:  29.30\n",
            "          Test  Loss: 0.00105 Acc: 100.00\n",
            "          Updated Best Loss: 0.00105 Best Acc: 100.00\n",
            "Epoch  61 Train Loss: 0.09769 Acc:  33.93\n",
            "          Test  Loss: 0.00073 Acc: 100.00\n",
            "          Updated Best Loss: 0.00073 Best Acc: 100.00\n",
            "Epoch  62 Train Loss: 0.08697 Acc:  38.02\n",
            "          Test  Loss: 0.00059 Acc: 100.00\n",
            "          Updated Best Loss: 0.00059 Best Acc: 100.00\n",
            "Epoch  63 Train Loss: 0.07958 Acc:  41.47\n",
            "          Test  Loss: 0.00082 Acc:  99.90\n",
            "Epoch  64 Train Loss: 0.06853 Acc:  47.74\n",
            "          Test  Loss: 0.00035 Acc:  99.90\n",
            "Epoch  65 Train Loss: 0.06271 Acc:  50.09\n",
            "          Test  Loss: 0.00026 Acc: 100.00\n",
            "          Updated Best Loss: 0.00026 Best Acc: 100.00\n",
            "Epoch  66 Train Loss: 0.05628 Acc:  55.07\n",
            "          Test  Loss: 0.00026 Acc:  99.90\n",
            "Epoch  67 Train Loss: 0.04991 Acc:  59.04\n",
            "          Test  Loss: 0.00019 Acc: 100.00\n",
            "          Updated Best Loss: 0.00019 Best Acc: 100.00\n",
            "Epoch  68 Train Loss: 0.04676 Acc:  61.26\n",
            "          Test  Loss: 0.00014 Acc: 100.00\n",
            "          Updated Best Loss: 0.00014 Best Acc: 100.00\n",
            "Epoch  69 Train Loss: 0.04278 Acc:  63.58\n",
            "          Test  Loss: 0.00013 Acc: 100.00\n",
            "          Updated Best Loss: 0.00013 Best Acc: 100.00\n",
            "Epoch  70 Train Loss: 0.04002 Acc:  65.13\n",
            "          Test  Loss: 0.00022 Acc: 100.00\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(19)\n",
        "\n",
        "train_dataset = DigitDataset(num_data=10000, token_len=32, seed=109)\n",
        "test_dataset  = DigitDataset(num_data=1000,  token_len=32, seed=10)\n",
        "collate_fn = DigitDataset.collate_fn\n",
        "vocab_size = DigitDataset.vocab_size\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=collate_fn, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=100, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model=64, dim_feedforward=256,\n",
        "                         num_encoder_layers=2, num_decoder_layers=2)\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "trainer1 = Trainer(model, criterion, optimizer, scheduler=None, device=device)\n",
        "best_loss, best_acc, state_dict = trainer1.train(train_loader, test_loader, num_epochs=70)\n",
        "\n",
        "os.makedirs(\"./ckpt/\", exist_ok=True)\n",
        "torch.save(state_dict, \"./ckpt/digit.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Result 2.1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yESHiRQBetO3",
        "outputId": "d55b4114-50ec-4254-a202-e6994e772f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Test Loss: 0.00013 Test Acc: 100.00%\n",
            "Ex 1 75272572156310634062892099449445\n",
            "  => 75272572156310634062892099449445\n",
            "Ex 2 40893093795296628969192129729449\n",
            "  => 40893093795296628969192129729449\n",
            "Ex 3 44831826149189749644665636026822\n",
            "  => 44831826149189749644665636026822\n",
            "Ex 4 95075203081688426351440359569079\n",
            "  => 95075203081688426351440359569079\n",
            "Ex 5 38679382662739589748458598384869\n",
            "  => 38679382662739589748458598384869\n"
          ]
        }
      ],
      "source": [
        "ex_batch = next(iter(test_loader))\n",
        "ex_src, ex_src_mask = ex_batch[0][:5].to(device), None\n",
        "start_idx = Vocab.to_idx(test_loader.dataset.start_token)\n",
        "end_idx   = Vocab.to_idx(test_loader.dataset.end_token)\n",
        "trainer1.model_load_state_dict(torch.load(\"./ckpt/digit.pt\"))\n",
        "results = trainer1.predict(ex_src, ex_src_mask, start_idx, end_idx)\n",
        "print(f\"Best Test Loss: {best_loss:.5f} Test Acc: {best_acc * 100:6.2f}%\")\n",
        "for i, (src_str, pred_str) in enumerate(results):\n",
        "    print(f\"Ex {i+1} {src_str}\\n  => {pred_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFwaECYwetO3"
      },
      "source": [
        "\n",
        "> **Problem 2.2** *(6 points)*\n",
        "  Now, we will implement a bit more useful function, so-called spelling error correction.\n",
        "  Your job is to create a model whose input is a word with spelling errors, and the output is the spelling-corrected word.\n",
        "  Here, your vocabulary will be character instead of word.\n",
        "  You can create your own training data by using an existing text corpus as the target and inject noise into it to use it as the input.\n",
        "  You are free to use whichever text corpus you like.\n",
        "  If you can't think of one, please use context data in SQuAD Dataset (see Assignment 2).\n",
        "  Report accuracy in your own evaluation data (you will receive full credit as long as both the evaluation data and the accuracy are reasonable),\n",
        "  and also show 5 examples where it succeeds at correcting spelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UcCTDHDTetO3"
      },
      "outputs": [],
      "source": [
        "class TypoDataset(NLPDataset):\n",
        "    def __init__(self, num_data: int, split: str = \"train\", level: str = \"word\",\n",
        "                 noise_rate: float = 0.9, seed: int = 109):\n",
        "        super().__init__(seed=seed)\n",
        "        self.split = split\n",
        "        self.level = level\n",
        "        self.noise_rate = noise_rate\n",
        "        squad = datasets.load_dataset(\"squad\", split=(\"validation\" if split == \"valid\" else split))\n",
        "        sentences = []\n",
        "        for context in sorted(set(squad[\"context\"])):\n",
        "            sentences.extend([s.lower() for s in (context + \" \").split(\". \")\n",
        "                              if Vocab.is_valid_vocabs(s) and len(s) > 10])\n",
        "        if level == \"word\":\n",
        "            words = []\n",
        "            for sentence in sentences:\n",
        "                words.extend([w for w in sentence.split(\" \")\n",
        "                              if not Vocab.is_numeric_vocabs(w) and len(w) > 1])\n",
        "            words = sorted(set(words))\n",
        "            idxs = torch.randint(len(words), size=(num_data,), generator=self.rng)\n",
        "            self.raw_data = [words[i] for i in idxs]\n",
        "            should_perterb = (torch.rand((num_data,), generator=self.rng) < noise_rate).tolist()\n",
        "            self.source = [self.tokens_to_idx(self._perturb(s) if p else s)\n",
        "                           for s, p in zip(self.raw_data, should_perterb)]\n",
        "            self.target = [self.tokens_to_idx(s) for s in self.raw_data]\n",
        "        elif level == \"sentence\":\n",
        "            sentences = [s for s in sentences if len(s) <= 48]\n",
        "            idxs = torch.randint(len(sentences), size=(num_data,), generator=self.rng)\n",
        "            self.raw_data = [sentences[i] for i in idxs]\n",
        "            should_perterb = (torch.rand((num_data,), generator=self.rng) < noise_rate).tolist()\n",
        "            self.source = [self.tokens_to_idx(self._perturb(s) if p else s)\n",
        "                           for s, p in zip(self.raw_data, should_perterb)]\n",
        "            self.target = [self.tokens_to_idx(s) for s in self.raw_data]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid level: {level}\")\n",
        "\n",
        "    def _perturb(self, tokens: str):\n",
        "        token_len = len(tokens)\n",
        "        while True:\n",
        "            t = torch.randint(12, size=(1,), generator=self.rng).item()\n",
        "            i = torch.randint(token_len - 1, size=(1,), generator=self.rng).item()\n",
        "            if t < 3:  # Swap\n",
        "                return tokens[:i] + tokens[i + 1] + tokens[i] + tokens[i + 2:]\n",
        "            elif t < 6:  # Insert\n",
        "                v = Vocab.sample_chars(generator=self.rng)\n",
        "                return tokens[:i] + v + tokens[i:]\n",
        "            elif not tokens[i].isdigit():\n",
        "                if t < 9:  # Delete\n",
        "                    return tokens[:i] + tokens[i + 1:]\n",
        "                else:  # Replace\n",
        "                    v = Vocab.sample_chars(generator=self.rng)\n",
        "                    return tokens[:i] + v + tokens[i + 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hHAcEd0NetO3",
        "outputId": "4e0cecdd-feae-467c-f7f8-b303ecbada5b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aab261c4ade44e598936e5256de82f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 Train Loss: 1.69818 Acc:   0.06\n",
            "          Test  Loss: 1.38475 Acc:   0.13\n",
            "          Updated Best Loss: 1.38475 Best Acc:   0.13\n",
            "Epoch   2 Train Loss: 1.35688 Acc:   0.54\n",
            "          Test  Loss: 1.24622 Acc:   1.62\n",
            "          Updated Best Loss: 1.24622 Best Acc:   1.62\n",
            "Epoch   3 Train Loss: 1.25990 Acc:   1.21\n",
            "          Test  Loss: 1.17731 Acc:   2.81\n",
            "          Updated Best Loss: 1.17731 Best Acc:   2.81\n",
            "Epoch   4 Train Loss: 1.20120 Acc:   1.87\n",
            "          Test  Loss: 1.12798 Acc:   4.38\n",
            "          Updated Best Loss: 1.12798 Best Acc:   4.38\n",
            "Epoch   5 Train Loss: 1.16701 Acc:   2.52\n",
            "          Test  Loss: 1.10524 Acc:   5.24\n",
            "          Updated Best Loss: 1.10524 Best Acc:   5.24\n",
            "Epoch   6 Train Loss: 1.14137 Acc:   3.03\n",
            "          Test  Loss: 1.07963 Acc:   6.48\n",
            "          Updated Best Loss: 1.07963 Best Acc:   6.48\n",
            "Epoch   7 Train Loss: 1.12128 Acc:   3.51\n",
            "          Test  Loss: 1.05379 Acc:   7.56\n",
            "          Updated Best Loss: 1.05379 Best Acc:   7.56\n",
            "Epoch   8 Train Loss: 1.10912 Acc:   3.94\n",
            "          Test  Loss: 1.03533 Acc:   9.40\n",
            "          Updated Best Loss: 1.03533 Best Acc:   9.40\n",
            "Epoch   9 Train Loss: 1.09197 Acc:   4.52\n",
            "          Test  Loss: 1.02326 Acc:  10.81\n",
            "          Updated Best Loss: 1.02326 Best Acc:  10.81\n",
            "Epoch  10 Train Loss: 1.07840 Acc:   5.04\n",
            "          Test  Loss: 1.01564 Acc:  11.04\n",
            "          Updated Best Loss: 1.01564 Best Acc:  11.04\n",
            "Epoch  11 Train Loss: 1.07227 Acc:   5.34\n",
            "          Test  Loss: 1.00631 Acc:  12.64\n",
            "          Updated Best Loss: 1.00631 Best Acc:  12.64\n",
            "Epoch  12 Train Loss: 1.06037 Acc:   6.01\n",
            "          Test  Loss: 0.99078 Acc:  14.42\n",
            "          Updated Best Loss: 0.99078 Best Acc:  14.42\n",
            "Epoch  13 Train Loss: 1.05077 Acc:   6.49\n",
            "          Test  Loss: 0.98997 Acc:  14.37\n",
            "Epoch  14 Train Loss: 1.04120 Acc:   7.03\n",
            "          Test  Loss: 0.97707 Acc:  15.99\n",
            "          Updated Best Loss: 0.97707 Best Acc:  15.99\n",
            "Epoch  15 Train Loss: 1.03267 Acc:   7.85\n",
            "          Test  Loss: 0.97170 Acc:  15.89\n",
            "Epoch  16 Train Loss: 1.01915 Acc:   8.59\n",
            "          Test  Loss: 0.94943 Acc:  20.26\n",
            "          Updated Best Loss: 0.94943 Best Acc:  20.26\n",
            "Epoch  17 Train Loss: 1.01505 Acc:   9.11\n",
            "          Test  Loss: 0.95211 Acc:  19.85\n",
            "Epoch  18 Train Loss: 1.00146 Acc:  10.14\n",
            "          Test  Loss: 0.93587 Acc:  22.26\n",
            "          Updated Best Loss: 0.93587 Best Acc:  22.26\n",
            "Epoch  19 Train Loss: 0.99214 Acc:  11.27\n",
            "          Test  Loss: 0.92890 Acc:  22.60\n",
            "          Updated Best Loss: 0.92890 Best Acc:  22.60\n",
            "Epoch  20 Train Loss: 0.98096 Acc:  12.32\n",
            "          Test  Loss: 0.92087 Acc:  24.58\n",
            "          Updated Best Loss: 0.92087 Best Acc:  24.58\n",
            "Epoch  21 Train Loss: 0.96709 Acc:  14.02\n",
            "          Test  Loss: 0.91283 Acc:  25.66\n",
            "          Updated Best Loss: 0.91283 Best Acc:  25.66\n",
            "Epoch  22 Train Loss: 0.95017 Acc:  15.92\n",
            "          Test  Loss: 0.89153 Acc:  29.34\n",
            "          Updated Best Loss: 0.89153 Best Acc:  29.34\n",
            "Epoch  23 Train Loss: 0.93791 Acc:  17.39\n",
            "          Test  Loss: 0.88487 Acc:  30.35\n",
            "          Updated Best Loss: 0.88487 Best Acc:  30.35\n",
            "Epoch  24 Train Loss: 0.93023 Acc:  18.87\n",
            "          Test  Loss: 0.89088 Acc:  28.48\n",
            "Epoch  25 Train Loss: 0.91720 Acc:  20.33\n",
            "          Test  Loss: 0.87702 Acc:  31.02\n",
            "          Updated Best Loss: 0.87702 Best Acc:  31.02\n",
            "Epoch  26 Train Loss: 0.90382 Acc:  22.05\n",
            "          Test  Loss: 0.87116 Acc:  32.19\n",
            "          Updated Best Loss: 0.87116 Best Acc:  32.19\n",
            "Epoch  27 Train Loss: 0.89662 Acc:  22.83\n",
            "          Test  Loss: 0.86139 Acc:  32.68\n",
            "          Updated Best Loss: 0.86139 Best Acc:  32.68\n",
            "Epoch  28 Train Loss: 0.88803 Acc:  24.70\n",
            "          Test  Loss: 0.85954 Acc:  33.32\n",
            "          Updated Best Loss: 0.85954 Best Acc:  33.32\n",
            "Epoch  29 Train Loss: 0.88331 Acc:  25.39\n",
            "          Test  Loss: 0.85532 Acc:  34.01\n",
            "          Updated Best Loss: 0.85532 Best Acc:  34.01\n",
            "Epoch  30 Train Loss: 0.87518 Acc:  26.76\n",
            "          Test  Loss: 0.85636 Acc:  33.91\n",
            "Epoch  31 Train Loss: 0.87161 Acc:  26.98\n",
            "          Test  Loss: 0.84652 Acc:  35.04\n",
            "          Updated Best Loss: 0.84652 Best Acc:  35.04\n",
            "Epoch  32 Train Loss: 0.86517 Acc:  28.35\n",
            "          Test  Loss: 0.84491 Acc:  35.74\n",
            "          Updated Best Loss: 0.84491 Best Acc:  35.74\n",
            "Epoch  33 Train Loss: 0.85718 Acc:  29.93\n",
            "          Test  Loss: 0.83931 Acc:  36.72\n",
            "          Updated Best Loss: 0.83931 Best Acc:  36.72\n",
            "Epoch  34 Train Loss: 0.85368 Acc:  30.69\n",
            "          Test  Loss: 0.84296 Acc:  35.51\n",
            "Epoch  35 Train Loss: 0.84986 Acc:  31.28\n",
            "          Test  Loss: 0.83829 Acc:  36.62\n",
            "Epoch  36 Train Loss: 0.84409 Acc:  32.55\n",
            "          Test  Loss: 0.83850 Acc:  36.95\n",
            "          Updated Best Loss: 0.83850 Best Acc:  36.95\n",
            "Epoch  37 Train Loss: 0.84139 Acc:  32.55\n",
            "          Test  Loss: 0.83107 Acc:  38.30\n",
            "          Updated Best Loss: 0.83107 Best Acc:  38.30\n",
            "Epoch  38 Train Loss: 0.84037 Acc:  33.24\n",
            "          Test  Loss: 0.83552 Acc:  37.41\n",
            "Epoch  39 Train Loss: 0.83670 Acc:  33.88\n",
            "          Test  Loss: 0.83491 Acc:  38.89\n",
            "          Updated Best Loss: 0.83491 Best Acc:  38.89\n",
            "Epoch  40 Train Loss: 0.83456 Acc:  34.57\n",
            "          Test  Loss: 0.83048 Acc:  38.84\n",
            "Epoch  41 Train Loss: 0.83319 Acc:  34.89\n",
            "          Test  Loss: 0.83051 Acc:  38.35\n",
            "Epoch  42 Train Loss: 0.82981 Acc:  35.77\n",
            "          Test  Loss: 0.82579 Acc:  39.77\n",
            "          Updated Best Loss: 0.82579 Best Acc:  39.77\n",
            "Epoch  43 Train Loss: 0.82771 Acc:  36.06\n",
            "          Test  Loss: 0.83098 Acc:  38.65\n",
            "Epoch  44 Train Loss: 0.82510 Acc:  36.76\n",
            "          Test  Loss: 0.82719 Acc:  39.61\n",
            "Epoch  45 Train Loss: 0.82500 Acc:  37.10\n",
            "          Test  Loss: 0.82638 Acc:  39.36\n",
            "Epoch  46 Train Loss: 0.82353 Acc:  37.40\n",
            "          Test  Loss: 0.82433 Acc:  41.12\n",
            "          Updated Best Loss: 0.82433 Best Acc:  41.12\n",
            "Epoch  47 Train Loss: 0.82312 Acc:  37.39\n",
            "          Test  Loss: 0.82446 Acc:  40.22\n",
            "Epoch  48 Train Loss: 0.82183 Acc:  37.97\n",
            "          Test  Loss: 0.82196 Acc:  40.83\n",
            "Epoch  49 Train Loss: 0.82037 Acc:  38.46\n",
            "          Test  Loss: 0.82227 Acc:  40.82\n",
            "Epoch  50 Train Loss: 0.81786 Acc:  38.97\n",
            "          Test  Loss: 0.81993 Acc:  41.95\n",
            "          Updated Best Loss: 0.81993 Best Acc:  41.95\n",
            "Epoch  51 Train Loss: 0.81691 Acc:  39.63\n",
            "          Test  Loss: 0.82041 Acc:  41.14\n",
            "Epoch  52 Train Loss: 0.81356 Acc:  39.92\n",
            "          Test  Loss: 0.81869 Acc:  42.49\n",
            "          Updated Best Loss: 0.81869 Best Acc:  42.49\n",
            "Epoch  53 Train Loss: 0.81413 Acc:  40.31\n",
            "          Test  Loss: 0.82024 Acc:  41.68\n",
            "Epoch  54 Train Loss: 0.81323 Acc:  40.46\n",
            "          Test  Loss: 0.81797 Acc:  42.81\n",
            "          Updated Best Loss: 0.81797 Best Acc:  42.81\n",
            "Epoch  55 Train Loss: 0.81007 Acc:  41.31\n",
            "          Test  Loss: 0.81786 Acc:  43.21\n",
            "          Updated Best Loss: 0.81786 Best Acc:  43.21\n",
            "Epoch  56 Train Loss: 0.81042 Acc:  41.58\n",
            "          Test  Loss: 0.81495 Acc:  42.99\n",
            "Epoch  57 Train Loss: 0.80738 Acc:  42.05\n",
            "          Test  Loss: 0.81758 Acc:  42.84\n",
            "Epoch  58 Train Loss: 0.80695 Acc:  42.14\n",
            "          Test  Loss: 0.81613 Acc:  43.14\n",
            "Epoch  59 Train Loss: 0.80484 Acc:  43.10\n",
            "          Test  Loss: 0.81352 Acc:  44.25\n",
            "          Updated Best Loss: 0.81352 Best Acc:  44.25\n",
            "Epoch  60 Train Loss: 0.80393 Acc:  43.32\n",
            "          Test  Loss: 0.81435 Acc:  43.81\n",
            "Epoch  61 Train Loss: 0.80492 Acc:  43.29\n",
            "          Test  Loss: 0.81369 Acc:  43.96\n",
            "Epoch  62 Train Loss: 0.80140 Acc:  44.05\n",
            "          Test  Loss: 0.81212 Acc:  44.54\n",
            "          Updated Best Loss: 0.81212 Best Acc:  44.54\n",
            "Epoch  63 Train Loss: 0.79998 Acc:  44.68\n",
            "          Test  Loss: 0.81418 Acc:  44.63\n",
            "          Updated Best Loss: 0.81418 Best Acc:  44.63\n",
            "Epoch  64 Train Loss: 0.79993 Acc:  45.17\n",
            "          Test  Loss: 0.81340 Acc:  44.44\n",
            "Epoch  65 Train Loss: 0.79941 Acc:  45.14\n",
            "          Test  Loss: 0.81208 Acc:  44.75\n",
            "          Updated Best Loss: 0.81208 Best Acc:  44.75\n",
            "Epoch  66 Train Loss: 0.79772 Acc:  45.84\n",
            "          Test  Loss: 0.81065 Acc:  45.64\n",
            "          Updated Best Loss: 0.81065 Best Acc:  45.64\n",
            "Epoch  67 Train Loss: 0.79474 Acc:  46.72\n",
            "          Test  Loss: 0.81052 Acc:  44.83\n",
            "Epoch  68 Train Loss: 0.79566 Acc:  46.70\n",
            "          Test  Loss: 0.81188 Acc:  45.65\n",
            "          Updated Best Loss: 0.81188 Best Acc:  45.65\n",
            "Epoch  69 Train Loss: 0.79532 Acc:  46.66\n",
            "          Test  Loss: 0.80974 Acc:  45.98\n",
            "          Updated Best Loss: 0.80974 Best Acc:  45.98\n",
            "Epoch  70 Train Loss: 0.79327 Acc:  47.32\n",
            "          Test  Loss: 0.81030 Acc:  45.74\n",
            "Epoch  71 Train Loss: 0.79217 Acc:  47.66\n",
            "          Test  Loss: 0.80944 Acc:  46.35\n",
            "          Updated Best Loss: 0.80944 Best Acc:  46.35\n",
            "Epoch  72 Train Loss: 0.79092 Acc:  48.11\n",
            "          Test  Loss: 0.81173 Acc:  45.64\n",
            "Epoch  73 Train Loss: 0.78956 Acc:  48.82\n",
            "          Test  Loss: 0.81151 Acc:  45.98\n",
            "Epoch  74 Train Loss: 0.78870 Acc:  49.02\n",
            "          Test  Loss: 0.80896 Acc:  46.98\n",
            "          Updated Best Loss: 0.80896 Best Acc:  46.98\n",
            "Epoch  75 Train Loss: 0.78759 Acc:  49.51\n",
            "          Test  Loss: 0.80944 Acc:  47.41\n",
            "          Updated Best Loss: 0.80944 Best Acc:  47.41\n",
            "Epoch  76 Train Loss: 0.78716 Acc:  50.09\n",
            "          Test  Loss: 0.80841 Acc:  46.67\n",
            "Epoch  77 Train Loss: 0.78594 Acc:  50.09\n",
            "          Test  Loss: 0.80941 Acc:  46.27\n",
            "Epoch  78 Train Loss: 0.78506 Acc:  50.78\n",
            "          Test  Loss: 0.80908 Acc:  46.47\n",
            "Epoch  79 Train Loss: 0.78446 Acc:  51.03\n",
            "          Test  Loss: 0.80748 Acc:  47.29\n",
            "Epoch  80 Train Loss: 0.78300 Acc:  51.63\n",
            "          Test  Loss: 0.80950 Acc:  46.48\n",
            "Epoch  81 Train Loss: 0.78238 Acc:  51.58\n",
            "          Test  Loss: 0.80832 Acc:  47.26\n",
            "Epoch  82 Train Loss: 0.78129 Acc:  52.17\n",
            "          Test  Loss: 0.80719 Acc:  48.08\n",
            "          Updated Best Loss: 0.80719 Best Acc:  48.08\n",
            "Epoch  83 Train Loss: 0.78101 Acc:  52.29\n",
            "          Test  Loss: 0.80752 Acc:  47.58\n",
            "Epoch  84 Train Loss: 0.78097 Acc:  52.54\n",
            "          Test  Loss: 0.80663 Acc:  47.48\n",
            "Epoch  85 Train Loss: 0.77907 Acc:  53.13\n",
            "          Test  Loss: 0.80794 Acc:  47.17\n",
            "Epoch  86 Train Loss: 0.77757 Acc:  53.80\n",
            "          Test  Loss: 0.80557 Acc:  47.82\n",
            "Epoch  87 Train Loss: 0.77852 Acc:  53.60\n",
            "          Test  Loss: 0.80637 Acc:  48.22\n",
            "          Updated Best Loss: 0.80637 Best Acc:  48.22\n",
            "Epoch  88 Train Loss: 0.77631 Acc:  54.29\n",
            "          Test  Loss: 0.80815 Acc:  47.19\n",
            "Epoch  89 Train Loss: 0.77663 Acc:  54.52\n",
            "          Test  Loss: 0.80540 Acc:  48.39\n",
            "          Updated Best Loss: 0.80540 Best Acc:  48.39\n",
            "Epoch  90 Train Loss: 0.77493 Acc:  55.20\n",
            "          Test  Loss: 0.80688 Acc:  47.99\n",
            "Epoch  91 Train Loss: 0.77405 Acc:  55.47\n",
            "          Test  Loss: 0.80777 Acc:  47.58\n",
            "Epoch  92 Train Loss: 0.77357 Acc:  56.00\n",
            "          Test  Loss: 0.80635 Acc:  48.31\n",
            "Epoch  93 Train Loss: 0.77218 Acc:  56.16\n",
            "          Test  Loss: 0.80845 Acc:  47.00\n",
            "Epoch  94 Train Loss: 0.77231 Acc:  56.05\n",
            "          Test  Loss: 0.80623 Acc:  48.13\n",
            "Epoch  95 Train Loss: 0.76990 Acc:  57.07\n",
            "          Test  Loss: 0.80523 Acc:  48.41\n",
            "          Updated Best Loss: 0.80523 Best Acc:  48.41\n",
            "Epoch  96 Train Loss: 0.76897 Acc:  57.46\n",
            "          Test  Loss: 0.80751 Acc:  47.86\n",
            "Epoch  97 Train Loss: 0.76885 Acc:  57.97\n",
            "          Test  Loss: 0.80810 Acc:  47.75\n",
            "Epoch  98 Train Loss: 0.76883 Acc:  57.59\n",
            "          Test  Loss: 0.80647 Acc:  48.71\n",
            "          Updated Best Loss: 0.80647 Best Acc:  48.71\n",
            "Epoch  99 Train Loss: 0.76733 Acc:  58.55\n",
            "          Test  Loss: 0.80687 Acc:  48.48\n",
            "Epoch 100 Train Loss: 0.76773 Acc:  58.55\n",
            "          Test  Loss: 0.80668 Acc:  47.88\n",
            "Epoch 101 Train Loss: 0.76609 Acc:  58.67\n",
            "          Test  Loss: 0.80625 Acc:  48.88\n",
            "          Updated Best Loss: 0.80625 Best Acc:  48.88\n",
            "Epoch 102 Train Loss: 0.76534 Acc:  59.06\n",
            "          Test  Loss: 0.80694 Acc:  48.18\n",
            "Epoch 103 Train Loss: 0.76542 Acc:  59.57\n",
            "          Test  Loss: 0.80900 Acc:  48.42\n",
            "Epoch 104 Train Loss: 0.76507 Acc:  59.80\n",
            "          Test  Loss: 0.80621 Acc:  49.11\n",
            "          Updated Best Loss: 0.80621 Best Acc:  49.11\n",
            "Epoch 105 Train Loss: 0.76386 Acc:  60.13\n",
            "          Test  Loss: 0.80537 Acc:  49.16\n",
            "          Updated Best Loss: 0.80537 Best Acc:  49.16\n",
            "Epoch 106 Train Loss: 0.76263 Acc:  60.89\n",
            "          Test  Loss: 0.80520 Acc:  48.94\n",
            "Epoch 107 Train Loss: 0.76244 Acc:  60.74\n",
            "          Test  Loss: 0.80440 Acc:  49.30\n",
            "          Updated Best Loss: 0.80440 Best Acc:  49.30\n",
            "Epoch 108 Train Loss: 0.76196 Acc:  61.12\n",
            "          Test  Loss: 0.80424 Acc:  49.01\n",
            "Epoch 109 Train Loss: 0.76218 Acc:  61.57\n",
            "          Test  Loss: 0.80538 Acc:  48.96\n",
            "Epoch 110 Train Loss: 0.76037 Acc:  62.08\n",
            "          Test  Loss: 0.80516 Acc:  49.33\n",
            "          Updated Best Loss: 0.80516 Best Acc:  49.33\n",
            "Epoch 111 Train Loss: 0.76048 Acc:  61.92\n",
            "          Test  Loss: 0.80587 Acc:  49.29\n",
            "Epoch 112 Train Loss: 0.75986 Acc:  62.38\n",
            "          Test  Loss: 0.80586 Acc:  49.69\n",
            "          Updated Best Loss: 0.80586 Best Acc:  49.69\n",
            "Epoch 113 Train Loss: 0.75940 Acc:  62.52\n",
            "          Test  Loss: 0.80362 Acc:  49.80\n",
            "          Updated Best Loss: 0.80362 Best Acc:  49.80\n",
            "Epoch 114 Train Loss: 0.75955 Acc:  62.62\n",
            "          Test  Loss: 0.80569 Acc:  50.06\n",
            "          Updated Best Loss: 0.80569 Best Acc:  50.06\n",
            "Epoch 115 Train Loss: 0.75705 Acc:  63.51\n",
            "          Test  Loss: 0.80533 Acc:  49.90\n",
            "Epoch 116 Train Loss: 0.75626 Acc:  64.20\n",
            "          Test  Loss: 0.80490 Acc:  49.90\n",
            "Epoch 117 Train Loss: 0.75649 Acc:  63.58\n",
            "          Test  Loss: 0.80582 Acc:  49.61\n",
            "Epoch 118 Train Loss: 0.75639 Acc:  63.64\n",
            "          Test  Loss: 0.80628 Acc:  48.99\n",
            "Epoch 119 Train Loss: 0.75675 Acc:  64.20\n",
            "          Test  Loss: 0.80666 Acc:  49.27\n",
            "Epoch 120 Train Loss: 0.75493 Acc:  64.81\n",
            "          Test  Loss: 0.80577 Acc:  49.80\n",
            "Epoch 121 Train Loss: 0.75441 Acc:  64.99\n",
            "          Test  Loss: 0.80658 Acc:  49.70\n",
            "Epoch 122 Train Loss: 0.75418 Acc:  65.03\n",
            "          Test  Loss: 0.80466 Acc:  50.40\n",
            "          Updated Best Loss: 0.80466 Best Acc:  50.40\n",
            "Epoch 123 Train Loss: 0.75388 Acc:  65.11\n",
            "          Test  Loss: 0.80767 Acc:  49.83\n",
            "Epoch 124 Train Loss: 0.75230 Acc:  65.81\n",
            "          Test  Loss: 0.80635 Acc:  50.13\n",
            "Epoch 125 Train Loss: 0.75229 Acc:  65.91\n",
            "          Test  Loss: 0.80793 Acc:  49.37\n",
            "Epoch 126 Train Loss: 0.75216 Acc:  65.94\n",
            "          Test  Loss: 0.80747 Acc:  49.78\n",
            "Epoch 127 Train Loss: 0.75178 Acc:  66.39\n",
            "          Test  Loss: 0.80640 Acc:  50.10\n",
            "Epoch 128 Train Loss: 0.75100 Acc:  66.50\n",
            "          Test  Loss: 0.80677 Acc:  50.11\n",
            "Epoch 129 Train Loss: 0.75063 Acc:  67.18\n",
            "          Test  Loss: 0.80612 Acc:  50.23\n",
            "Epoch 130 Train Loss: 0.75052 Acc:  67.13\n",
            "          Test  Loss: 0.80744 Acc:  49.76\n",
            "Epoch 131 Train Loss: 0.74994 Acc:  67.06\n",
            "          Test  Loss: 0.80723 Acc:  50.06\n",
            "Epoch 132 Train Loss: 0.74959 Acc:  67.52\n",
            "          Test  Loss: 0.80594 Acc:  49.76\n",
            "Epoch 133 Train Loss: 0.74895 Acc:  68.05\n",
            "          Test  Loss: 0.80801 Acc:  50.30\n",
            "Epoch 134 Train Loss: 0.74998 Acc:  67.25\n",
            "          Test  Loss: 0.80815 Acc:  49.48\n",
            "Epoch 135 Train Loss: 0.74861 Acc:  68.08\n",
            "          Test  Loss: 0.80792 Acc:  50.57\n",
            "          Updated Best Loss: 0.80792 Best Acc:  50.57\n",
            "Epoch 136 Train Loss: 0.74789 Acc:  68.61\n",
            "          Test  Loss: 0.80817 Acc:  49.77\n",
            "Epoch 137 Train Loss: 0.74701 Acc:  68.95\n",
            "          Test  Loss: 0.80804 Acc:  50.35\n",
            "Epoch 138 Train Loss: 0.74575 Acc:  69.60\n",
            "          Test  Loss: 0.80799 Acc:  50.17\n",
            "Epoch 139 Train Loss: 0.74551 Acc:  69.67\n",
            "          Test  Loss: 0.80675 Acc:  50.06\n",
            "Epoch 140 Train Loss: 0.74686 Acc:  69.10\n",
            "          Test  Loss: 0.81003 Acc:  49.60\n",
            "Epoch 141 Train Loss: 0.74621 Acc:  69.42\n",
            "          Test  Loss: 0.80704 Acc:  50.60\n",
            "          Updated Best Loss: 0.80704 Best Acc:  50.60\n",
            "Epoch 142 Train Loss: 0.74510 Acc:  70.14\n",
            "          Test  Loss: 0.80846 Acc:  50.46\n",
            "Epoch 143 Train Loss: 0.74558 Acc:  69.71\n",
            "          Test  Loss: 0.80764 Acc:  50.24\n",
            "Epoch 144 Train Loss: 0.74441 Acc:  70.38\n",
            "          Test  Loss: 0.80765 Acc:  50.63\n",
            "          Updated Best Loss: 0.80765 Best Acc:  50.63\n",
            "Epoch 145 Train Loss: 0.74453 Acc:  70.38\n",
            "          Test  Loss: 0.80890 Acc:  50.27\n",
            "Epoch 146 Train Loss: 0.74271 Acc:  71.13\n",
            "          Test  Loss: 0.80691 Acc:  50.65\n",
            "          Updated Best Loss: 0.80691 Best Acc:  50.65\n",
            "Epoch 147 Train Loss: 0.74368 Acc:  70.73\n",
            "          Test  Loss: 0.80783 Acc:  50.74\n",
            "          Updated Best Loss: 0.80783 Best Acc:  50.74\n",
            "Epoch 148 Train Loss: 0.74307 Acc:  71.04\n",
            "          Test  Loss: 0.80899 Acc:  50.04\n",
            "Epoch 149 Train Loss: 0.74275 Acc:  71.36\n",
            "          Test  Loss: 0.80793 Acc:  50.64\n",
            "Epoch 150 Train Loss: 0.74245 Acc:  71.35\n",
            "          Test  Loss: 0.80911 Acc:  50.52\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(19)\n",
        "\n",
        "train_dataset = TypoDataset(num_data=51200, level=\"word\", split=\"train\", seed=109)\n",
        "test_dataset  = TypoDataset(num_data=10240, level=\"word\", split=\"valid\", seed=10)\n",
        "collate_fn = TypoDataset.collate_fn\n",
        "vocab_size = TypoDataset.vocab_size\n",
        "clear_output()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=collate_fn,\n",
        "                          num_workers=2, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=256, collate_fn=collate_fn,\n",
        "                          num_workers=2, shuffle=False)\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model=256, dim_feedforward=1024,\n",
        "                         num_encoder_layers=2, num_decoder_layers=2)\n",
        "criterion = CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
        "scheduler = NoamLR(optimizer, warmup_epochs=30)\n",
        "\n",
        "trainer2 = Trainer(model, criterion, optimizer, scheduler, device)\n",
        "best_loss, best_acc, state_dict = trainer2.train(train_loader, test_loader, num_epochs=150)\n",
        "\n",
        "os.makedirs(\"./ckpt/\", exist_ok=True)\n",
        "torch.save(state_dict, \"./ckpt/typo_word.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Result 2.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hvKW8JcEetO4",
        "outputId": "350e5abd-6f9b-400a-f6df-c297dbfebfaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Test Loss: 0.80783 Test Acc:  50.74%\n",
            "Ex  1: takigg => taking\n",
            "Ex  2: injruy => injury\n",
            "Ex  3: offic5ial => official\n",
            "Ex  4: suqare => square\n",
            "Ex  5: actdor => actor\n",
            "Ex  6: prkpagation => propagation\n",
            "Ex  7: obstacel => obstacle\n",
            "Ex  8: autsralia => australia\n"
          ]
        }
      ],
      "source": [
        "ex_batch = next(iter(test_loader))\n",
        "ex_idx = [9, 35, 47, 69, 70, 76, 78, 95]\n",
        "ex_src = ex_batch[0][ex_idx].to(device)\n",
        "ex_src_mask = ex_batch[2][ex_idx].to(device)\n",
        "start_idx = Vocab.to_idx(test_loader.dataset.start_token)\n",
        "end_idx   = Vocab.to_idx(test_loader.dataset.end_token)\n",
        "trainer2.model_load_state_dict(torch.load(\"./ckpt/typo_word.pt\"))\n",
        "results = trainer2.predict(ex_src, ex_src_mask, start_idx, end_idx)\n",
        "print(f\"Best Test Loss: {best_loss:.5f} Test Acc: {best_acc * 100:6.2f}%\")\n",
        "for i, (src_str, pred_str) in enumerate(results):\n",
        "    print(f\"Ex {i+1:2d}: {src_str} => {pred_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJf8sSuetO4"
      },
      "source": [
        "> **Problem 2.3 (bonus)** *(3 points)*\n",
        "  Extend this word-level spelling correction model to sentence-level.\n",
        "  You do not have to report accuracy, but find 3 examples where the word-level model fails and sentence-level model correctly predicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DGO2fx3SetO4",
        "outputId": "d5412308-043c-45b9-8502-9fbe6ed00ed5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b4ca28f1677438bb395edda3efc0cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 Train Loss: 2.42568 Acc:   0.00\n",
            "          Test  Loss: 2.09924 Acc:   0.00\n",
            "          Updated Best Loss: 2.09924 Best Acc:   0.00\n",
            "Epoch   2 Train Loss: 2.08601 Acc:   0.00\n",
            "          Test  Loss: 1.97111 Acc:   0.00\n",
            "          Updated Best Loss: 1.97111 Best Acc:   0.00\n",
            "Epoch   3 Train Loss: 1.95894 Acc:   0.00\n",
            "          Test  Loss: 1.84959 Acc:   0.00\n",
            "          Updated Best Loss: 1.84959 Best Acc:   0.00\n",
            "Epoch   4 Train Loss: 1.83685 Acc:   0.00\n",
            "          Test  Loss: 1.75349 Acc:   0.00\n",
            "          Updated Best Loss: 1.75349 Best Acc:   0.00\n",
            "Epoch   5 Train Loss: 1.73483 Acc:   0.00\n",
            "          Test  Loss: 1.68858 Acc:   0.00\n",
            "          Updated Best Loss: 1.68858 Best Acc:   0.00\n",
            "Epoch   6 Train Loss: 1.65211 Acc:   0.01\n",
            "          Test  Loss: 1.63354 Acc:   0.00\n",
            "          Updated Best Loss: 1.63354 Best Acc:   0.00\n",
            "Epoch   7 Train Loss: 1.58170 Acc:   0.04\n",
            "          Test  Loss: 1.61131 Acc:   0.00\n",
            "          Updated Best Loss: 1.61131 Best Acc:   0.00\n",
            "Epoch   8 Train Loss: 1.51686 Acc:   0.10\n",
            "          Test  Loss: 1.61329 Acc:   0.02\n",
            "          Updated Best Loss: 1.61329 Best Acc:   0.02\n",
            "Epoch   9 Train Loss: 1.45647 Acc:   0.15\n",
            "          Test  Loss: 1.57306 Acc:   0.03\n",
            "          Updated Best Loss: 1.57306 Best Acc:   0.03\n",
            "Epoch  10 Train Loss: 1.39401 Acc:   0.22\n",
            "          Test  Loss: 1.58412 Acc:   0.20\n",
            "          Updated Best Loss: 1.58412 Best Acc:   0.20\n",
            "Epoch  11 Train Loss: 1.33131 Acc:   0.37\n",
            "          Test  Loss: 1.59089 Acc:   0.00\n",
            "Epoch  12 Train Loss: 1.27091 Acc:   0.52\n",
            "          Test  Loss: 1.61221 Acc:   0.05\n",
            "Epoch  13 Train Loss: 1.21098 Acc:   0.66\n",
            "          Test  Loss: 1.63732 Acc:   0.22\n",
            "          Updated Best Loss: 1.63732 Best Acc:   0.22\n",
            "Epoch  14 Train Loss: 1.15274 Acc:   0.99\n",
            "          Test  Loss: 1.65542 Acc:   0.20\n",
            "Epoch  15 Train Loss: 1.09600 Acc:   1.61\n",
            "          Test  Loss: 1.68658 Acc:   0.39\n",
            "          Updated Best Loss: 1.68658 Best Acc:   0.39\n",
            "Epoch  16 Train Loss: 1.05128 Acc:   2.27\n",
            "          Test  Loss: 1.70099 Acc:   0.26\n",
            "Epoch  17 Train Loss: 1.00794 Acc:   3.16\n",
            "          Test  Loss: 1.72330 Acc:   0.64\n",
            "          Updated Best Loss: 1.72330 Best Acc:   0.64\n",
            "Epoch  18 Train Loss: 0.97136 Acc:   4.48\n",
            "          Test  Loss: 1.72189 Acc:   0.71\n",
            "          Updated Best Loss: 1.72189 Best Acc:   0.71\n",
            "Epoch  19 Train Loss: 0.94342 Acc:   5.96\n",
            "          Test  Loss: 1.72928 Acc:   0.78\n",
            "          Updated Best Loss: 1.72928 Best Acc:   0.78\n",
            "Epoch  20 Train Loss: 0.91961 Acc:   7.18\n",
            "          Test  Loss: 1.75287 Acc:   0.89\n",
            "          Updated Best Loss: 1.75287 Best Acc:   0.89\n",
            "Epoch  21 Train Loss: 0.89698 Acc:   9.36\n",
            "          Test  Loss: 1.74520 Acc:   0.90\n",
            "          Updated Best Loss: 1.74520 Best Acc:   0.90\n",
            "Epoch  22 Train Loss: 0.88197 Acc:  10.75\n",
            "          Test  Loss: 1.74811 Acc:   0.85\n",
            "Epoch  23 Train Loss: 0.86807 Acc:  11.99\n",
            "          Test  Loss: 1.74585 Acc:   0.60\n",
            "Epoch  24 Train Loss: 0.85296 Acc:  13.95\n",
            "          Test  Loss: 1.75940 Acc:   0.74\n",
            "Epoch  25 Train Loss: 0.84663 Acc:  14.73\n",
            "          Test  Loss: 1.73661 Acc:   0.80\n",
            "Epoch  26 Train Loss: 0.83165 Acc:  17.02\n",
            "          Test  Loss: 1.73345 Acc:   0.95\n",
            "          Updated Best Loss: 1.73345 Best Acc:   0.95\n",
            "Epoch  27 Train Loss: 0.82710 Acc:  17.27\n",
            "          Test  Loss: 1.70684 Acc:   0.80\n",
            "Epoch  28 Train Loss: 0.81941 Acc:  18.80\n",
            "          Test  Loss: 1.68386 Acc:   1.06\n",
            "          Updated Best Loss: 1.68386 Best Acc:   1.06\n",
            "Epoch  29 Train Loss: 0.81020 Acc:  20.58\n",
            "          Test  Loss: 1.65118 Acc:   1.05\n",
            "Epoch  30 Train Loss: 0.80566 Acc:  21.10\n",
            "          Test  Loss: 1.60461 Acc:   1.90\n",
            "          Updated Best Loss: 1.60461 Best Acc:   1.90\n",
            "Epoch  31 Train Loss: 0.79705 Acc:  23.10\n",
            "          Test  Loss: 1.54804 Acc:   1.70\n",
            "Epoch  32 Train Loss: 0.79124 Acc:  24.79\n",
            "          Test  Loss: 1.48194 Acc:   1.30\n",
            "Epoch  33 Train Loss: 0.78613 Acc:  26.33\n",
            "          Test  Loss: 1.41244 Acc:   1.81\n",
            "Epoch  34 Train Loss: 0.78406 Acc:  26.79\n",
            "          Test  Loss: 1.33526 Acc:   1.63\n",
            "Epoch  35 Train Loss: 0.77795 Acc:  28.51\n",
            "          Test  Loss: 1.36252 Acc:   1.98\n",
            "          Updated Best Loss: 1.36252 Best Acc:   1.98\n",
            "Epoch  36 Train Loss: 0.77825 Acc:  29.35\n",
            "          Test  Loss: 1.22054 Acc:   2.84\n",
            "          Updated Best Loss: 1.22054 Best Acc:   2.84\n",
            "Epoch  37 Train Loss: 0.76787 Acc:  32.70\n",
            "          Test  Loss: 1.15589 Acc:   2.04\n",
            "Epoch  38 Train Loss: 0.76676 Acc:  33.17\n",
            "          Test  Loss: 1.07734 Acc:   3.12\n",
            "          Updated Best Loss: 1.07734 Best Acc:   3.12\n",
            "Epoch  39 Train Loss: 0.76363 Acc:  34.96\n",
            "          Test  Loss: 1.01561 Acc:   4.41\n",
            "          Updated Best Loss: 1.01561 Best Acc:   4.41\n",
            "Epoch  40 Train Loss: 0.76041 Acc:  36.82\n",
            "          Test  Loss: 0.95387 Acc:   5.82\n",
            "          Updated Best Loss: 0.95387 Best Acc:   5.82\n",
            "Epoch  41 Train Loss: 0.76000 Acc:  36.82\n",
            "          Test  Loss: 0.90033 Acc:   8.41\n",
            "          Updated Best Loss: 0.90033 Best Acc:   8.41\n",
            "Epoch  42 Train Loss: 0.75955 Acc:  37.41\n",
            "          Test  Loss: 0.87646 Acc:   9.52\n",
            "          Updated Best Loss: 0.87646 Best Acc:   9.52\n",
            "Epoch  43 Train Loss: 0.75937 Acc:  37.45\n",
            "          Test  Loss: 0.84871 Acc:  14.61\n",
            "          Updated Best Loss: 0.84871 Best Acc:  14.61\n",
            "Epoch  44 Train Loss: 0.75914 Acc:  37.67\n",
            "          Test  Loss: 0.81681 Acc:  18.55\n",
            "          Updated Best Loss: 0.81681 Best Acc:  18.55\n",
            "Epoch  45 Train Loss: 0.75886 Acc:  37.79\n",
            "          Test  Loss: 0.80431 Acc:  21.21\n",
            "          Updated Best Loss: 0.80431 Best Acc:  21.21\n",
            "Epoch  46 Train Loss: 0.75809 Acc:  37.43\n",
            "          Test  Loss: 0.79100 Acc:  25.64\n",
            "          Updated Best Loss: 0.79100 Best Acc:  25.64\n",
            "Epoch  47 Train Loss: 0.75881 Acc:  36.59\n",
            "          Test  Loss: 0.78114 Acc:  28.54\n",
            "          Updated Best Loss: 0.78114 Best Acc:  28.54\n",
            "Epoch  48 Train Loss: 0.75922 Acc:  36.04\n",
            "          Test  Loss: 0.78179 Acc:  26.67\n",
            "Epoch  49 Train Loss: 0.75913 Acc:  36.12\n",
            "          Test  Loss: 0.77649 Acc:  29.70\n",
            "          Updated Best Loss: 0.77649 Best Acc:  29.70\n",
            "Epoch  50 Train Loss: 0.76052 Acc:  34.88\n",
            "          Test  Loss: 0.76435 Acc:  33.99\n",
            "          Updated Best Loss: 0.76435 Best Acc:  33.99\n",
            "Epoch  51 Train Loss: 0.76064 Acc:  34.52\n",
            "          Test  Loss: 0.76569 Acc:  31.87\n",
            "Epoch  52 Train Loss: 0.76022 Acc:  34.52\n",
            "          Test  Loss: 0.76360 Acc:  33.22\n",
            "Epoch  53 Train Loss: 0.76040 Acc:  34.53\n",
            "          Test  Loss: 0.76336 Acc:  34.29\n",
            "          Updated Best Loss: 0.76336 Best Acc:  34.29\n",
            "Epoch  54 Train Loss: 0.76075 Acc:  34.38\n",
            "          Test  Loss: 0.76227 Acc:  34.71\n",
            "          Updated Best Loss: 0.76227 Best Acc:  34.71\n",
            "Epoch  55 Train Loss: 0.76010 Acc:  34.27\n",
            "          Test  Loss: 0.76056 Acc:  34.05\n",
            "Epoch  56 Train Loss: 0.75981 Acc:  34.47\n",
            "          Test  Loss: 0.75938 Acc:  35.55\n",
            "          Updated Best Loss: 0.75938 Best Acc:  35.55\n",
            "Epoch  57 Train Loss: 0.75960 Acc:  34.40\n",
            "          Test  Loss: 0.76436 Acc:  34.17\n",
            "Epoch  58 Train Loss: 0.75965 Acc:  34.64\n",
            "          Test  Loss: 0.75986 Acc:  34.13\n",
            "Epoch  59 Train Loss: 0.75892 Acc:  34.85\n",
            "          Test  Loss: 0.75755 Acc:  36.69\n",
            "          Updated Best Loss: 0.75755 Best Acc:  36.69\n",
            "Epoch  60 Train Loss: 0.75885 Acc:  34.81\n",
            "          Test  Loss: 0.75924 Acc:  34.65\n",
            "Epoch  61 Train Loss: 0.75831 Acc:  35.24\n",
            "          Test  Loss: 0.75847 Acc:  36.29\n",
            "Epoch  62 Train Loss: 0.75794 Acc:  35.38\n",
            "          Test  Loss: 0.75834 Acc:  35.91\n",
            "Epoch  63 Train Loss: 0.75713 Acc:  35.96\n",
            "          Test  Loss: 0.75724 Acc:  37.06\n",
            "          Updated Best Loss: 0.75724 Best Acc:  37.06\n",
            "Epoch  64 Train Loss: 0.75726 Acc:  35.78\n",
            "          Test  Loss: 0.75708 Acc:  36.41\n",
            "Epoch  65 Train Loss: 0.75660 Acc:  36.25\n",
            "          Test  Loss: 0.75790 Acc:  36.92\n",
            "Epoch  66 Train Loss: 0.75656 Acc:  36.28\n",
            "          Test  Loss: 0.75795 Acc:  36.21\n",
            "Epoch  67 Train Loss: 0.75610 Acc:  36.72\n",
            "          Test  Loss: 0.75733 Acc:  38.24\n",
            "          Updated Best Loss: 0.75733 Best Acc:  38.24\n",
            "Epoch  68 Train Loss: 0.75504 Acc:  37.29\n",
            "          Test  Loss: 0.75821 Acc:  36.39\n",
            "Epoch  69 Train Loss: 0.75519 Acc:  37.35\n",
            "          Test  Loss: 0.75760 Acc:  38.24\n",
            "Epoch  70 Train Loss: 0.75471 Acc:  37.74\n",
            "          Test  Loss: 0.75366 Acc:  38.90\n",
            "          Updated Best Loss: 0.75366 Best Acc:  38.90\n",
            "Epoch  71 Train Loss: 0.75445 Acc:  37.67\n",
            "          Test  Loss: 0.75903 Acc:  37.77\n",
            "Epoch  72 Train Loss: 0.75398 Acc:  38.17\n",
            "          Test  Loss: 0.75756 Acc:  37.89\n",
            "Epoch  73 Train Loss: 0.75372 Acc:  38.52\n",
            "          Test  Loss: 0.75756 Acc:  38.02\n",
            "Epoch  74 Train Loss: 0.75345 Acc:  38.42\n",
            "          Test  Loss: 0.75453 Acc:  38.99\n",
            "          Updated Best Loss: 0.75453 Best Acc:  38.99\n",
            "Epoch  75 Train Loss: 0.75309 Acc:  38.84\n",
            "          Test  Loss: 0.75446 Acc:  39.05\n",
            "          Updated Best Loss: 0.75446 Best Acc:  39.05\n",
            "Epoch  76 Train Loss: 0.75308 Acc:  38.78\n",
            "          Test  Loss: 0.75741 Acc:  37.63\n",
            "Epoch  77 Train Loss: 0.75232 Acc:  39.28\n",
            "          Test  Loss: 0.75728 Acc:  37.87\n",
            "Epoch  78 Train Loss: 0.75233 Acc:  39.47\n",
            "          Test  Loss: 0.75625 Acc:  38.84\n",
            "Epoch  79 Train Loss: 0.75178 Acc:  39.78\n",
            "          Test  Loss: 0.75649 Acc:  38.24\n",
            "Epoch  80 Train Loss: 0.75161 Acc:  39.89\n",
            "          Test  Loss: 0.75415 Acc:  39.32\n",
            "          Updated Best Loss: 0.75415 Best Acc:  39.32\n",
            "Epoch  81 Train Loss: 0.75160 Acc:  39.93\n",
            "          Test  Loss: 0.75491 Acc:  41.04\n",
            "          Updated Best Loss: 0.75491 Best Acc:  41.04\n",
            "Epoch  82 Train Loss: 0.75114 Acc:  40.13\n",
            "          Test  Loss: 0.75762 Acc:  39.79\n",
            "Epoch  83 Train Loss: 0.75069 Acc:  40.60\n",
            "          Test  Loss: 0.75523 Acc:  39.08\n",
            "Epoch  84 Train Loss: 0.75059 Acc:  40.50\n",
            "          Test  Loss: 0.75337 Acc:  41.31\n",
            "          Updated Best Loss: 0.75337 Best Acc:  41.31\n",
            "Epoch  85 Train Loss: 0.75010 Acc:  40.87\n",
            "          Test  Loss: 0.75529 Acc:  39.36\n",
            "Epoch  86 Train Loss: 0.74970 Acc:  41.27\n",
            "          Test  Loss: 0.75534 Acc:  41.30\n",
            "Epoch  87 Train Loss: 0.74978 Acc:  41.17\n",
            "          Test  Loss: 0.75384 Acc:  41.07\n",
            "Epoch  88 Train Loss: 0.74988 Acc:  41.20\n",
            "          Test  Loss: 0.75363 Acc:  40.15\n",
            "Epoch  89 Train Loss: 0.74940 Acc:  41.46\n",
            "          Test  Loss: 0.75418 Acc:  41.25\n",
            "Epoch  90 Train Loss: 0.74877 Acc:  41.90\n",
            "          Test  Loss: 0.75171 Acc:  41.08\n",
            "Epoch  91 Train Loss: 0.74865 Acc:  42.10\n",
            "          Test  Loss: 0.75670 Acc:  39.39\n",
            "Epoch  92 Train Loss: 0.74890 Acc:  41.74\n",
            "          Test  Loss: 0.75501 Acc:  41.46\n",
            "          Updated Best Loss: 0.75501 Best Acc:  41.46\n",
            "Epoch  93 Train Loss: 0.74812 Acc:  42.22\n",
            "          Test  Loss: 0.75393 Acc:  40.20\n",
            "Epoch  94 Train Loss: 0.74786 Acc:  42.62\n",
            "          Test  Loss: 0.75624 Acc:  40.25\n",
            "Epoch  95 Train Loss: 0.74813 Acc:  42.38\n",
            "          Test  Loss: 0.75494 Acc:  41.53\n",
            "          Updated Best Loss: 0.75494 Best Acc:  41.53\n",
            "Epoch  96 Train Loss: 0.74752 Acc:  43.09\n",
            "          Test  Loss: 0.75336 Acc:  42.11\n",
            "          Updated Best Loss: 0.75336 Best Acc:  42.11\n",
            "Epoch  97 Train Loss: 0.74749 Acc:  42.87\n",
            "          Test  Loss: 0.75576 Acc:  40.21\n",
            "Epoch  98 Train Loss: 0.74722 Acc:  43.03\n",
            "          Test  Loss: 0.75549 Acc:  40.16\n",
            "Epoch  99 Train Loss: 0.74715 Acc:  43.41\n",
            "          Test  Loss: 0.75374 Acc:  41.11\n",
            "Epoch 100 Train Loss: 0.74661 Acc:  43.56\n",
            "          Test  Loss: 0.75364 Acc:  41.91\n",
            "Epoch 101 Train Loss: 0.74701 Acc:  43.33\n",
            "          Test  Loss: 0.75397 Acc:  41.61\n",
            "Epoch 102 Train Loss: 0.74652 Acc:  43.74\n",
            "          Test  Loss: 0.75715 Acc:  39.98\n",
            "Epoch 103 Train Loss: 0.74656 Acc:  43.68\n",
            "          Test  Loss: 0.75678 Acc:  40.58\n",
            "Epoch 104 Train Loss: 0.74630 Acc:  43.82\n",
            "          Test  Loss: 0.75700 Acc:  39.85\n",
            "Epoch 105 Train Loss: 0.74607 Acc:  44.12\n",
            "          Test  Loss: 0.75368 Acc:  42.18\n",
            "          Updated Best Loss: 0.75368 Best Acc:  42.18\n",
            "Epoch 106 Train Loss: 0.74593 Acc:  44.20\n",
            "          Test  Loss: 0.75453 Acc:  41.46\n",
            "Epoch 107 Train Loss: 0.74590 Acc:  44.13\n",
            "          Test  Loss: 0.75661 Acc:  40.58\n",
            "Epoch 108 Train Loss: 0.74543 Acc:  44.51\n",
            "          Test  Loss: 0.75613 Acc:  40.22\n",
            "Epoch 109 Train Loss: 0.74535 Acc:  44.52\n",
            "          Test  Loss: 0.75624 Acc:  41.87\n",
            "Epoch 110 Train Loss: 0.74531 Acc:  44.62\n",
            "          Test  Loss: 0.75567 Acc:  40.72\n",
            "Epoch 111 Train Loss: 0.74508 Acc:  45.13\n",
            "          Test  Loss: 0.75688 Acc:  41.80\n",
            "Epoch 112 Train Loss: 0.74451 Acc:  45.29\n",
            "          Test  Loss: 0.75550 Acc:  39.96\n",
            "Epoch 113 Train Loss: 0.74451 Acc:  45.37\n",
            "          Test  Loss: 0.75525 Acc:  40.23\n",
            "Epoch 114 Train Loss: 0.74453 Acc:  45.13\n",
            "          Test  Loss: 0.75705 Acc:  40.26\n",
            "Epoch 115 Train Loss: 0.74427 Acc:  45.59\n",
            "          Test  Loss: 0.75605 Acc:  40.40\n",
            "Epoch 116 Train Loss: 0.74465 Acc:  45.39\n",
            "          Test  Loss: 0.75623 Acc:  40.49\n",
            "Epoch 117 Train Loss: 0.74409 Acc:  45.66\n",
            "          Test  Loss: 0.75494 Acc:  41.76\n",
            "Epoch 118 Train Loss: 0.74389 Acc:  45.57\n",
            "          Test  Loss: 0.75833 Acc:  38.67\n",
            "Epoch 119 Train Loss: 0.74401 Acc:  45.71\n",
            "          Test  Loss: 0.75706 Acc:  41.43\n",
            "Epoch 120 Train Loss: 0.74346 Acc:  46.08\n",
            "          Test  Loss: 0.75363 Acc:  40.02\n",
            "Epoch 121 Train Loss: 0.74336 Acc:  46.12\n",
            "          Test  Loss: 0.75471 Acc:  41.10\n",
            "Epoch 122 Train Loss: 0.74314 Acc:  46.34\n",
            "          Test  Loss: 0.75489 Acc:  42.12\n",
            "Epoch 123 Train Loss: 0.74330 Acc:  46.18\n",
            "          Test  Loss: 0.75628 Acc:  40.98\n",
            "Epoch 124 Train Loss: 0.74318 Acc:  46.50\n",
            "          Test  Loss: 0.75490 Acc:  40.97\n",
            "Epoch 125 Train Loss: 0.74311 Acc:  46.52\n",
            "          Test  Loss: 0.75790 Acc:  40.84\n",
            "Epoch 126 Train Loss: 0.74269 Acc:  46.80\n",
            "          Test  Loss: 0.75570 Acc:  41.42\n",
            "Epoch 127 Train Loss: 0.74289 Acc:  46.62\n",
            "          Test  Loss: 0.75438 Acc:  41.96\n",
            "Epoch 128 Train Loss: 0.74271 Acc:  46.70\n",
            "          Test  Loss: 0.75712 Acc:  41.49\n",
            "Epoch 129 Train Loss: 0.74236 Acc:  47.05\n",
            "          Test  Loss: 0.75501 Acc:  41.54\n",
            "Epoch 130 Train Loss: 0.74246 Acc:  46.84\n",
            "          Test  Loss: 0.75770 Acc:  40.27\n",
            "Epoch 131 Train Loss: 0.74232 Acc:  47.11\n",
            "          Test  Loss: 0.75845 Acc:  41.77\n",
            "Epoch 132 Train Loss: 0.74220 Acc:  47.26\n",
            "          Test  Loss: 0.75490 Acc:  41.81\n",
            "Epoch 133 Train Loss: 0.74171 Acc:  47.67\n",
            "          Test  Loss: 0.75820 Acc:  40.80\n",
            "Epoch 134 Train Loss: 0.74171 Acc:  47.31\n",
            "          Test  Loss: 0.75901 Acc:  39.92\n",
            "Epoch 135 Train Loss: 0.74181 Acc:  47.48\n",
            "          Test  Loss: 0.75815 Acc:  41.08\n",
            "Epoch 136 Train Loss: 0.74160 Acc:  47.72\n",
            "          Test  Loss: 0.75673 Acc:  40.62\n",
            "Epoch 137 Train Loss: 0.74162 Acc:  47.74\n",
            "          Test  Loss: 0.75515 Acc:  41.61\n",
            "Epoch 138 Train Loss: 0.74147 Acc:  47.44\n",
            "          Test  Loss: 0.75527 Acc:  42.84\n",
            "          Updated Best Loss: 0.75527 Best Acc:  42.84\n",
            "Epoch 139 Train Loss: 0.74135 Acc:  47.89\n",
            "          Test  Loss: 0.75688 Acc:  40.92\n",
            "Epoch 140 Train Loss: 0.74119 Acc:  48.04\n",
            "          Test  Loss: 0.75548 Acc:  40.63\n",
            "Epoch 141 Train Loss: 0.74088 Acc:  48.41\n",
            "          Test  Loss: 0.75683 Acc:  40.55\n",
            "Epoch 142 Train Loss: 0.74083 Acc:  48.20\n",
            "          Test  Loss: 0.75695 Acc:  41.23\n",
            "Epoch 143 Train Loss: 0.74105 Acc:  48.27\n",
            "          Test  Loss: 0.75697 Acc:  41.18\n",
            "Epoch 144 Train Loss: 0.74084 Acc:  48.37\n",
            "          Test  Loss: 0.75941 Acc:  40.21\n",
            "Epoch 145 Train Loss: 0.74080 Acc:  48.45\n",
            "          Test  Loss: 0.75674 Acc:  41.47\n",
            "Epoch 146 Train Loss: 0.74048 Acc:  48.45\n",
            "          Test  Loss: 0.75668 Acc:  40.95\n",
            "Epoch 147 Train Loss: 0.74089 Acc:  48.00\n",
            "          Test  Loss: 0.75646 Acc:  40.48\n",
            "Epoch 148 Train Loss: 0.74033 Acc:  48.65\n",
            "          Test  Loss: 0.75429 Acc:  42.35\n",
            "Epoch 149 Train Loss: 0.73998 Acc:  48.76\n",
            "          Test  Loss: 0.75934 Acc:  41.37\n",
            "Epoch 150 Train Loss: 0.74036 Acc:  48.77\n",
            "          Test  Loss: 0.75503 Acc:  41.65\n",
            "Epoch 151 Train Loss: 0.74006 Acc:  48.94\n",
            "          Test  Loss: 0.75831 Acc:  40.08\n",
            "Epoch 152 Train Loss: 0.73976 Acc:  49.20\n",
            "          Test  Loss: 0.75862 Acc:  41.11\n",
            "Epoch 153 Train Loss: 0.73987 Acc:  48.93\n",
            "          Test  Loss: 0.75718 Acc:  42.43\n",
            "Epoch 154 Train Loss: 0.73993 Acc:  49.25\n",
            "          Test  Loss: 0.75761 Acc:  40.35\n",
            "Epoch 155 Train Loss: 0.73974 Acc:  49.20\n",
            "          Test  Loss: 0.75866 Acc:  40.16\n",
            "Epoch 156 Train Loss: 0.73975 Acc:  49.25\n",
            "          Test  Loss: 0.75823 Acc:  41.30\n",
            "Epoch 157 Train Loss: 0.73961 Acc:  49.19\n",
            "          Test  Loss: 0.76029 Acc:  41.42\n",
            "Epoch 158 Train Loss: 0.73952 Acc:  49.41\n",
            "          Test  Loss: 0.75643 Acc:  42.00\n",
            "Epoch 159 Train Loss: 0.73928 Acc:  49.60\n",
            "          Test  Loss: 0.75545 Acc:  41.78\n",
            "Epoch 160 Train Loss: 0.73922 Acc:  49.79\n",
            "          Test  Loss: 0.75746 Acc:  40.71\n",
            "Epoch 161 Train Loss: 0.73884 Acc:  49.86\n",
            "          Test  Loss: 0.76052 Acc:  39.86\n",
            "Epoch 162 Train Loss: 0.73903 Acc:  49.74\n",
            "          Test  Loss: 0.76138 Acc:  40.03\n",
            "Epoch 163 Train Loss: 0.73919 Acc:  49.52\n",
            "          Test  Loss: 0.75857 Acc:  40.60\n",
            "Epoch 164 Train Loss: 0.73887 Acc:  50.13\n",
            "          Test  Loss: 0.75762 Acc:  41.26\n",
            "Epoch 165 Train Loss: 0.73890 Acc:  50.10\n",
            "          Test  Loss: 0.75653 Acc:  41.71\n",
            "Epoch 166 Train Loss: 0.73856 Acc:  50.16\n",
            "          Test  Loss: 0.75663 Acc:  41.81\n",
            "Epoch 167 Train Loss: 0.73847 Acc:  50.42\n",
            "          Test  Loss: 0.75615 Acc:  41.60\n",
            "Epoch 168 Train Loss: 0.73855 Acc:  50.45\n",
            "          Test  Loss: 0.75597 Acc:  42.94\n",
            "          Updated Best Loss: 0.75597 Best Acc:  42.94\n",
            "Epoch 169 Train Loss: 0.73848 Acc:  50.42\n",
            "          Test  Loss: 0.75564 Acc:  41.82\n",
            "Epoch 170 Train Loss: 0.73862 Acc:  50.19\n",
            "          Test  Loss: 0.75910 Acc:  40.86\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(19)\n",
        "\n",
        "train_dataset = TypoDataset(num_data=51200, level=\"sentence\", split=\"train\", seed=109)\n",
        "test_dataset  = TypoDataset(num_data=10240, level=\"sentence\", split=\"valid\", seed=10)\n",
        "collate_fn = TypoDataset.collate_fn\n",
        "vocab_size = TypoDataset.vocab_size\n",
        "clear_output()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=collate_fn,\n",
        "                          num_workers=2, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=256, collate_fn=collate_fn,\n",
        "                          num_workers=2, shuffle=False)\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model=256, dim_feedforward=1024,\n",
        "                         num_encoder_layers=6, num_decoder_layers=6)\n",
        "criterion = CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "scheduler = NoamLR(optimizer, warmup_epochs=50)\n",
        "\n",
        "trainer3 = Trainer(model, criterion, optimizer, scheduler, device)\n",
        "best_loss, best_acc, state_dict = trainer3.train(train_loader, test_loader, num_epochs=170)\n",
        "\n",
        "os.makedirs(\"./ckpt/\", exist_ok=True)\n",
        "torch.save(state_dict, \"./ckpt/typo_sentence.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Result 2.3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aFehr3y2etO5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Test Loss: 0.75597 Test Acc:  42.94%\n",
            "Ex  1: he died inn 1259 without a successor\n",
            "    => he died in 1259 without a successor\n",
            "Ex  2: mongol rule wtas cosmopolitan under kublai khan\n",
            "    => mongol rule was cosmopolitan under kublai khan\n",
            "Ex  3: regional variations and dishes alsu exist\n",
            "    => regional variations and dishes also exist\n",
            "Ex  4: he came back5 to lahore in 1908\n",
            "    => he came back to lahore in 1908\n",
            "Ex  5: johns river divides the czty\n",
            "    => johns river divides the city\n",
            "Ex  6: theey claimed the law infringed article 34\n",
            "    => they claimed the law infringed article 34\n",
            "Ex  7: sweden rationed gasoline and hetaing oil\n",
            "    => sweden rationed gasoline and heating oil\n",
            "Ex  8: thits was derogatory in the consumers' eyes\n",
            "    => this was derogatory in the consumers' eyes\n"
          ]
        }
      ],
      "source": [
        "ex_batch = next(iter(test_loader))\n",
        "ex_idx = [7, 29, 31, 47, 54, 88, 98, 121]\n",
        "ex_src = ex_batch[0][ex_idx].to(device)\n",
        "ex_src_mask = ex_batch[2][ex_idx].to(device)\n",
        "start_idx = Vocab.to_idx(test_loader.dataset.start_token)\n",
        "end_idx   = Vocab.to_idx(test_loader.dataset.end_token)\n",
        "trainer3.model_load_state_dict(torch.load(\"./ckpt/typo_sentence.pt\"))\n",
        "results = trainer3.predict(ex_src, ex_src_mask, start_idx, end_idx)\n",
        "print(f\"Best Test Loss: {best_loss:.5f} Test Acc: {best_acc * 100:6.2f}%\")\n",
        "for i, (src_str, pred_str) in enumerate(results):\n",
        "    print(f\"Ex {i+1:2d}: {src_str}\\n    => {pred_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ex  1: inn => inn\n",
            "Ex  2: wtas => wats\n",
            "Ex  3: alsu => alus\n",
            "Ex  4: back5 => backl\n",
            "Ex  5: czty => coty\n",
            "Ex  6: theey => they\n",
            "Ex  7: hetaing => hettaing\n",
            "Ex  8: thits => thirts\n"
          ]
        }
      ],
      "source": [
        "src = [test_dataset.tokens_to_idx(s) for s in [\n",
        "    \"inn\", \"wtas\", \"alsu\", \"back5\", \"czty\", \"theey\", \"hetaing\", \"thits\",\n",
        "]]\n",
        "ex_src, _, ex_src_mask, _ = collate_fn(list(zip(src, src)))\n",
        "results = trainer2.predict(ex_src.to(device), ex_src_mask.to(device), start_idx, end_idx)\n",
        "for i, (src_str, pred_str) in enumerate(results):\n",
        "    print(f\"Ex {i+1:2d}: {src_str} => {pred_str}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
